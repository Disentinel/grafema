# T1.1: Segment Format (Track 1, Rust)

> Milestone: M1 (Foundation)
> Dependencies: none
> Estimated: ~1800 LOC, ~45 tests
> Related docs: [002-roadmap.md](../002-roadmap.md) Phase 0, [004-expert-concerns.md](../004-expert-concerns.md) I3/I4/N8

---

## High-Level Context

Это **фундамент всего RFDB v2**. Иммутабельный колоночный сегмент — атомарный строительный блок, на котором строятся все последующие фазы.

Текущий формат v1 (`storage/segment.rs`, ~490 LOC) уже колоночный и mmap-based, но:
- Нет `semantic_id` как колонки (строковый ID хранится в metadata hack)
- Нет `content_hash` колонки (I4)
- Нет bloom filter (edge dedup через HashSet в памяти)
- Нет zone maps (I3)
- Нет dst bloom filter на edge сегментах (N8)
- Footer отсутствует — string table привязана к фиксированному offset в header

v2 формат решает все эти проблемы + закладывает extensibility через footer.

---

## Existing Code to Study

| File | What to learn |
|------|--------------|
| `src/storage/segment.rs` (~490 LOC) | v1 segment format, mmap pattern, columnar layout |
| `src/storage/string_table.rs` (~163 LOC) | String interning, dedup, binary format |
| `src/storage/writer.rs` (~402 LOC) | Segment serialization, flush logic |
| `src/storage/mod.rs` (~161 LOC) | `NodeRecord`, `EdgeRecord` struct definitions |
| `src/graph/id_gen.rs` (~120 LOC) | BLAKE3 hashing, `string_id_to_u128()` |

---

## New Module: `storage_v2/`

```
src/storage_v2/
  mod.rs            ← re-exports
  segment.rs        ← NodeSegmentV2, EdgeSegmentV2 (read + types)
  writer.rs         ← SegmentWriter (write path)
  bloom.rs          ← BloomFilter (src bloom + dst bloom)
  zone_map.rs       ← ZoneMap (per-field distinct value sets)
  string_table.rs   ← StringTableV2 (evolved from v1, same concept)
  types.rs          ← NodeRecordV2, EdgeRecordV2
```

---

## Data Structures

### NodeRecordV2

```rust
/// Node record for v2 storage
/// Key difference from v1: semantic_id is first-class, content_hash is explicit
pub struct NodeRecordV2 {
    /// Semantic ID string — THE identity
    /// Format: "file->TYPE->name[in:namedParent]" (see 007-semantic-id-stability-research.md)
    pub semantic_id: String,

    /// BLAKE3(semantic_id) → u128, derived index for fast lookup
    /// ALWAYS computed from semantic_id, never independent
    pub id: u128,

    /// Node type: "FUNCTION", "CLASS", "http:route", etc.
    pub node_type: String,

    /// Entity name
    pub name: String,

    /// File path
    pub file: String,

    /// Content hash: xxHash64 of source text span
    /// Computed by analyzer, NOT by RFDB
    /// Used for: precision diff (I4), analyzer coverage canary
    pub content_hash: u64,

    /// JSON metadata string (everything else)
    /// NO originalId, NO _origSrc/_origDst — those hacks are gone
    pub metadata: String,
}
```

**Nuances:**
- `id` MUST always equal `BLAKE3(semantic_id)`. Compute at write time, verify at read time (debug builds).
- `content_hash = 0` means "analyzer didn't compute hash" (valid for non-source nodes like EXTERNAL_MODULE).
- `metadata` can be empty string `""` — NOT `"{}"`. Empty = no metadata. Saves 2 bytes per node × millions.
- `node_type` is NOT optional (unlike v1 where it was `Option<String>`). Every node has a type.

### EdgeRecordV2

```rust
pub struct EdgeRecordV2 {
    /// Source node u128 (BLAKE3 of semantic_id)
    pub src: u128,

    /// Destination node u128
    pub dst: u128,

    /// Edge type: "CALLS", "CONTAINS", "IMPORTS_FROM", etc.
    pub edge_type: String,

    /// JSON metadata string
    pub metadata: String,
}
```

**Nuances:**
- No `version` field (v1 had it). Version = snapshot number in manifest.
- No `deleted` flag on individual records. Tombstones are separate (Phase 4).
- src/dst are u128 only. String resolution happens at API boundary (Decision #4 in roadmap).

---

## Segment Binary Format

### Header (fixed, 32 bytes)

```
Offset  Size  Field
0       4     magic: [u8; 4] = b"SGV2"  (Segment V2, distinct from v1 "SGRF")
4       2     version: u16 = 2
6       1     segment_type: u8 = 0 (nodes) | 1 (edges)
7       1     reserved: u8 = 0
8       8     record_count: u64
16      8     footer_offset: u64         ← byte offset to footer start
24      8     reserved: u64 = 0
```

**Nuances:**
- Magic `SGV2` vs v1 `SGRF` — instant format detection without version check.
- `footer_offset` is critical: footer contains bloom, zone maps, string table offsets.
  Reader opens file, reads header, jumps to footer, gets all metadata.
- Header is exactly 32 bytes (power of 2, cache-line friendly).

### Node Columns (immediately after header)

```
Offset 32: [semantic_id offsets: u32 × record_count]  ← offsets into string table
           [id column: u128 × record_count]
           [node_type offsets: u32 × record_count]     ← offsets into string table
           [name offsets: u32 × record_count]           ← offsets into string table
           [file offsets: u32 × record_count]           ← offsets into string table
           [content_hash column: u64 × record_count]
           [metadata offsets: u32 × record_count]       ← offsets into string table
```

**Nuances:**
- Column ORDER matters for mmap alignment. u128 (16 bytes) first after u32 arrays would cause alignment issues.
  Solution: u32 arrays first, then u128 (naturally aligned to 16), then u64.
  Alternatively: pad between columns. Decision: **group by size** — all u32 columns, then u128, then u64.

  Revised order:
  ```
  [semantic_id offsets: u32 × N]
  [node_type offsets: u32 × N]
  [name offsets: u32 × N]
  [file offsets: u32 × N]
  [metadata offsets: u32 × N]
  --- padding to 16-byte boundary ---
  [id column: u128 × N]
  [content_hash column: u64 × N]
  ```

- v1 has `exported` and `deleted` columns. v2 **drops both**:
  - `exported` → move to metadata (rarely queried directly)
  - `deleted` → tombstone segments (Phase 4), not per-record flag

### Edge Columns

```
[src column: u128 × record_count]
[dst column: u128 × record_count]
--- padding to 4-byte boundary if needed ---
[edge_type offsets: u32 × record_count]
[metadata offsets: u32 × record_count]
```

**Nuances:**
- Edge order: u128 first (src, dst), then u32 (type, metadata). Natural alignment.
- No `version`, no `deleted` (same reasoning as nodes).

### Footer

```
[bloom_filter: variable]                 ← src bloom (nodes: on id, edges: on src)
[dst_bloom_filter: variable]             ← edges only: bloom on dst field (N8)
[zone_maps: variable]                    ← per-field distinct value sets (I3)
[string_table: variable]                 ← all interned strings
[footer_index: fixed]                    ← offsets to each footer section
```

Footer index (last N bytes before EOF):
```
bloom_offset: u64
dst_bloom_offset: u64     (0 for node segments)
zone_maps_offset: u64
string_table_offset: u64
footer_index_magic: u32 = 0x46545232   ("FTR2")
```

**Nuances:**
- Footer index is at END of file. Reader: open file → seek to (EOF - 28) → read footer index → jump to sections.
- This is the Parquet/ORC pattern: header at start, footer index at end.
- `dst_bloom_offset = 0` for node segments (no dst field). Reader checks segment_type.

---

## Bloom Filter Spec

```rust
pub struct BloomFilter {
    bits: Vec<u64>,        // Bit array (64-bit words)
    num_bits: usize,       // Total bits
    num_hashes: usize,     // Number of hash functions (k)
}
```

Parameters:
- **10 bits per key** → ~1% FPR (standard choice)
- **k = 7** hash functions (optimal for 10 bits/key)
- Hash functions: use BLAKE3 to generate k hashes via double-hashing technique:
  `h_i(x) = (h1(x) + i * h2(x)) % num_bits` where h1, h2 from BLAKE3 output

**Nuances:**
- Input for node bloom: `u128` id (16 bytes → BLAKE3)
- Input for edge src bloom: `u128` src
- Input for edge dst bloom: `u128` dst (N8)
- Bloom filter size: for 1000 nodes × 10 bits = 10,000 bits = 1.25 KB per segment
- **Must handle empty segments**: 0 items → empty bloom (always returns false)
- Bloom is **immutable** once written. No insert after construction.

Binary format:
```
[num_bits: u64]
[num_hashes: u32]
[padding: u32]
[bits: u64 × ceil(num_bits / 64)]
```

---

## Zone Map Spec

```rust
/// Per-field distinct value tracking for segment skipping
pub struct ZoneMap {
    /// For each indexed field: set of distinct values in this segment
    fields: HashMap<String, HashSet<String>>,
}
```

Fields to track:
- `node_type`: set of all node types in segment (e.g., {"FUNCTION", "VARIABLE", "CALL"})
- `file`: set of all file paths in segment
- `edge_type`: set of all edge types in segment (edge segments only)

**Nuances:**
- Zone maps are **tiny**: 5-20 distinct node types, 1-10 files per segment (file-scoped shards!).
- For file-scoped shards (I2 decision), `file` zone map has exactly 1 value → trivial.
- Zone map query: "does this segment contain nodeType=FUNCTION?" → check zone map → skip if no.
- Built during write (free — already iterating records). Stored in footer.
- Zone maps don't replace bloom filters — they serve different purposes:
  - Bloom: "does id X exist in this segment?" (point lookup)
  - Zone map: "does any record with type=FUNCTION exist?" (attribute filter)

Binary format:
```
[field_count: u32]
For each field:
  [field_name_len: u16]
  [field_name: utf8 bytes]
  [value_count: u32]
  For each value:
    [value_len: u16]
    [value: utf8 bytes]
```

---

## String Table V2

Same concept as v1, minor improvements:
- Add `contains(s: &str) -> bool` method (for zone map construction)
- Consider `intern_or_get(s: &str) -> (u32, bool)` returning whether string was new

Keep binary format compatible with v1 for simplicity. No need to change it.

---

## Write Path: SegmentWriter

```rust
pub struct SegmentWriter {
    records: Vec<NodeRecordV2>,  // or Vec<EdgeRecordV2>
    string_table: StringTableV2,
}

impl SegmentWriter {
    pub fn new() -> Self;
    pub fn add_node(&mut self, node: NodeRecordV2);
    pub fn add_edge(&mut self, edge: EdgeRecordV2);

    /// Write segment to file. Returns segment metadata.
    pub fn write_nodes<W: Write + Seek>(self, writer: &mut W) -> Result<SegmentMeta>;
    pub fn write_edges<W: Write + Seek>(self, writer: &mut W) -> Result<SegmentMeta>;
}

pub struct SegmentMeta {
    pub record_count: u64,
    pub byte_size: u64,
    pub node_types: HashSet<String>,   // for manifest stats
    pub file_paths: HashSet<String>,   // for manifest stats
}
```

Write sequence:
1. Intern all strings into string table
2. Compute u128 ids from semantic_ids (BLAKE3)
3. Build bloom filter(s) from ids
4. Build zone maps from distinct values
5. Write header (with placeholder footer_offset)
6. Write column data
7. Write footer (bloom, dst_bloom, zone maps, string table)
8. Write footer index
9. Seek back to header, update footer_offset
10. Flush

**Nuances:**
- Step 9 requires `Seek`. For mmap-based writes, this is fine. For streaming writes, use a different approach (buffer header, write at end).
- String table must be built BEFORE columns (columns reference string offsets).
- Bloom filter must be built AFTER all records added (needs all keys).

---

## Read Path: NodeSegmentV2 / EdgeSegmentV2

```rust
pub struct NodeSegmentV2 {
    mmap: Mmap,
    header: SegmentHeaderV2,
    bloom: BloomFilter,        // loaded from footer
    zone_map: ZoneMap,         // loaded from footer
    // Column offsets (computed from header)
    semantic_id_offsets_start: usize,
    node_type_offsets_start: usize,
    name_offsets_start: usize,
    file_offsets_start: usize,
    metadata_offsets_start: usize,
    ids_start: usize,
    content_hash_start: usize,
    // String table
    string_table: StringTableV2,  // loaded from footer
}

impl NodeSegmentV2 {
    pub fn open(path: &Path) -> Result<Self>;

    // Point access by index
    pub fn get_id(&self, index: usize) -> u128;
    pub fn get_semantic_id(&self, index: usize) -> &str;
    pub fn get_node_type(&self, index: usize) -> &str;
    pub fn get_name(&self, index: usize) -> &str;
    pub fn get_file(&self, index: usize) -> &str;
    pub fn get_content_hash(&self, index: usize) -> u64;
    pub fn get_metadata(&self, index: usize) -> &str;

    // Full record reconstruction
    pub fn get_record(&self, index: usize) -> NodeRecordV2;

    // Bloom check
    pub fn maybe_contains(&self, id: u128) -> bool;

    // Zone map check
    pub fn contains_node_type(&self, node_type: &str) -> bool;
    pub fn contains_file(&self, file: &str) -> bool;

    // Scan
    pub fn iter(&self) -> impl Iterator<Item = NodeRecordV2>;
    pub fn record_count(&self) -> usize;
}
```

**Nuances:**
- `get_semantic_id()` returns `&str` pointing into mmap'd string table. Zero-copy.
- `open()` reads footer first (seek to EOF-28), then loads bloom, zone maps, string table.
- Column offsets computed arithmetically from header (no stored offsets needed — layout is fixed).
- `maybe_contains()` is the bloom filter check — "maybe yes" or "definitely no".

---

## Critical Nuances

### 1. Alignment
u128 columns must be 16-byte aligned for efficient reads. After writing all u32 columns, pad to 16-byte boundary before u128 column. The padding size depends on record_count.

```rust
fn compute_padding(offset: usize, alignment: usize) -> usize {
    let rem = offset % alignment;
    if rem == 0 { 0 } else { alignment - rem }
}
```

### 2. Empty segments
Must handle gracefully: 0 records → valid segment with empty columns, empty bloom (returns false for all), empty zone map. This happens when a file has no nodes of a certain type.

### 3. Large metadata
Metadata can be very large (full function signature, JSDoc, etc.). String table dedup helps but some metadata is unique per node. Plan for segments where metadata dominates byte count.

### 4. Endianness
All integers are **little-endian** (matches x86/ARM default, no conversion needed on common platforms). Document this explicitly. Use `u128::to_le_bytes()` / `from_le_bytes()`.

### 5. Corruption detection
- Verify magic bytes on open
- Verify footer_offset < file_size
- Verify record_count matches column sizes
- In debug mode: verify all string table offsets are within bounds
- NOT a full checksum (too expensive for mmap). Trust the filesystem.

### 6. mmap considerations
- Use `memmap2` crate (same as v1)
- `Mmap::map()` for read-only segments
- Pages faulted in on demand by OS — first access to a column may be slow
- `madvise(MADV_SEQUENTIAL)` for full scans, `MADV_RANDOM` for point lookups

### 7. v1 backward compat
v2 segments are NOT backward compatible with v1. Different magic, different layout. `NodeSegmentV2::open()` checks magic — if `SGRF` → error "v1 segment, use migration tool".

---

## Test Plan

### Property-based (proptest)
1. `write_read_roundtrip_nodes` — random Vec<NodeRecordV2> → write → read → identical
2. `write_read_roundtrip_edges` — same for edges
3. `semantic_id_u128_derivation` — for all written nodes: BLAKE3(semantic_id) == stored u128
4. `content_hash_roundtrip` — content_hash survives roundtrip exactly
5. `string_table_dedup` — duplicate strings → single storage

### Bloom filter
6. `bloom_no_false_negatives` — every inserted key found
7. `bloom_fpr_under_2_percent` — random non-inserted keys: FPR < 2%
8. `bloom_empty` — empty bloom returns false for all
9. `bloom_single_item` — one item: found=true, others=false
10. `bloom_roundtrip` — serialize → deserialize → same results

### Zone maps
11. `zone_map_exact_values` — zone map contains exactly the distinct values
12. `zone_map_empty_segment` — empty zone map for empty segment
13. `zone_map_single_type` — one node type → one value in zone map
14. `zone_map_roundtrip` — serialize → deserialize → same

### Edge cases
15. `empty_segment` — 0 records: valid, readable, bloom=false, zone_map=empty
16. `single_record_segment` — 1 record: all columns accessible
17. `max_metadata_size` — node with 1MB metadata → roundtrip OK
18. `empty_metadata` — metadata="" → roundtrip as empty, not "{}"
19. `unicode_strings` — semantic_id with unicode → roundtrip OK
20. `very_long_semantic_id` — 500+ char ID → roundtrip OK

### Corruption
21. `truncated_file` → clean error
22. `wrong_magic` → clean error "not a v2 segment"
23. `v1_magic` → clean error "v1 segment, use migration"
24. `corrupted_footer_offset` → clean error
25. `zero_byte_file` → clean error

### Binary stability
26. `byte_exact_roundtrip` — write → read → write → byte-exact same file

### Benchmarks
27. `bench_write_throughput` — target: >500K nodes/sec
28. `bench_read_sequential` — sequential scan throughput
29. `bench_read_random` — random point lookups via bloom+scan
30. `bench_bloom_check` — bloom filter lookup latency

### Dst bloom (edge segments)
31. `dst_bloom_no_false_negatives` — every dst found
32. `dst_bloom_fpr` — FPR < 2%
33. `dst_bloom_independent` — src bloom and dst bloom are independent

### Alignment
34. `column_alignment` — u128 column starts at 16-byte aligned offset
35. `various_record_counts` — test with N=0,1,2,3,7,8,15,16,100,1000,10000

---

## Performance Targets

| Operation | Target | Notes |
|-----------|--------|-------|
| Write throughput | >500K nodes/sec | Single thread, SSD |
| Sequential scan | >1M records/sec | mmap sequential |
| Point lookup (bloom+scan) | <10 µs per query | After bloom filter check |
| Bloom filter check | <100 ns | In-memory, cached |
| Zone map check | <50 ns | In-memory, cached |
| Segment open | <1 ms | mmap + footer parse |

---

## Dependencies (Cargo)

```toml
blake3 = "1.5"       # ID computation (already in project)
memmap2 = "0.9"       # Memory-mapped files (already in project)
proptest = "1.4"      # Property-based testing (dev)
criterion = "0.5"     # Benchmarks (already in project)
xxhash-rust = "0.8"   # For content_hash verification (optional)
```
