# T2.3: Multi-Shard (Track 1, Rust)

> Milestone: M2 (Storage Engine)
> Dependencies: T1.1, T2.1, T2.2
> Estimated: ~800 LOC, ~20 tests
> Related docs: [002-roadmap.md](../002-roadmap.md) §2 (File Grouping), [004-expert-concerns.md](../004-expert-concerns.md) I2, [T2.2 spec](./T2.2-single-shard-read-write.md)

---

## High-Level Context

T2.2 даёт нам один шард (весь граф в одной директории сегментов). Для инкрементального обновления нужно **разделение по файлам** — изменение одного файла затрагивает только его шард, а не весь граф.

**Shard planner** распределяет файлы по шардам:
- Каждый файл проекта → ровно один шард
- Шард = directory с сегментами для subset файлов
- Queries fan-out по шардам + merge результатов

**Схема шардирования: directory-based.** Файлы в одной директории → один шард. Это минимизирует cross-shard edges (импорты обычно локальны) и стабильно при добавлении файлов.

**Зачем multi-shard:**
1. **Инкрементальность:** re-analyze file → tombstone только в его шарде → O(shard), не O(graph)
2. **Параллелизм:** разные шарды пишутся параллельно (rayon)
3. **Locality:** файлы одной директории рядом на диске → cache friendly

---

## Existing Code to Study

| File | What to learn |
|------|--------------|
| `src/storage_v2/shard.rs` | Shard struct from T2.2 |
| `src/graph/engine_v2.rs` | GraphEngineV2 single-shard from T2.2 |
| `src/storage_v2/manifest.rs` | ManifestStore from T2.1 |

---

## New & Modified Files

```
src/storage_v2/
  shard_planner.rs        ← NEW: file → shard assignment logic
src/graph/
  engine_v2.rs            ← MODIFY: multi-shard support
```

---

## Shard Planner

### Assignment Algorithm

```rust
/// Assigns files to shards based on directory structure.
///
/// Algorithm:
/// 1. Group files by parent directory
/// 2. Each directory with >0 files gets a shard
/// 3. Small directories (< MIN_FILES_PER_SHARD) merged with parent
///
/// Example:
///   src/auth/login.ts    → shard "src/auth"
///   src/auth/register.ts → shard "src/auth"
///   src/api/routes.ts    → shard "src/api"
///   src/api/handlers.ts  → shard "src/api"
///   src/app.ts           → shard "src"
pub struct ShardPlanner {
    /// Minimum files per shard before merging with parent
    min_files_per_shard: usize,
}

impl ShardPlanner {
    pub fn new(min_files_per_shard: usize) -> Self;

    /// Given list of file paths, return shard assignments.
    /// Returns: Map<shard_name, Vec<file_path>>
    pub fn plan(&self, files: &[String]) -> HashMap<String, Vec<String>>;

    /// Get shard name for a single file.
    pub fn shard_for_file(&self, file: &str) -> String;
}
```

**Default `min_files_per_shard = 3`.** A directory with 1-2 files gets merged up.

### Shard Naming

Shard name = directory path relative to project root, with `/` replaced by `_`:

```
src/auth/  →  shard name: "src_auth"
src/api/   →  shard name: "src_api"
src/       →  shard name: "src"
```

### Filesystem Layout

```
<name>.rfdb/
├── current.json
├── manifests/
├── shards/
│   ├── src_auth/
│   │   ├── seg_000001_nodes.bin
│   │   └── seg_000001_edges.bin
│   ├── src_api/
│   │   ├── seg_000002_nodes.bin
│   │   └── seg_000002_edges.bin
│   └── src/
│       ├── seg_000003_nodes.bin
│       └── seg_000003_edges.bin
├── gc/
└── field_declarations.json
```

### Manifest Updates for Multi-Shard

```rust
pub struct SegmentDescriptor {
    pub segment_id: u64,
    pub file_name: String,
    pub record_count: u64,
    pub byte_size: u64,
    pub node_types: Vec<String>,
    pub file_paths: Vec<String>,
    pub edge_types: Vec<String>,
    pub shard: String,           // ← NEW: shard name this segment belongs to
}
```

### Plan Stability

**Key property:** Adding a new file should NOT reassign existing files to different shards.

- New file → assigned to its directory's shard (or creates new shard if new directory)
- Existing files → unchanged
- Deleting a file → shard may become empty → cleaned up on GC

This is crucial for incremental updates: shard reassignment = full re-analysis of affected files.

---

## Multi-Shard Engine

### Modified GraphEngineV2

```rust
pub struct GraphEngineV2 {
    /// Shards indexed by shard name
    shards: HashMap<String, Shard>,

    /// Shard planner for file→shard routing
    shard_planner: ShardPlanner,

    /// Manifest store (shared across all shards)
    manifest_store: ManifestStore,

    /// Path to database
    db_path: PathBuf,
}
```

### Write Routing

```rust
impl GraphEngineV2 {
    fn add_nodes(&mut self, nodes: &[NodeRecordV2]) -> Result<()> {
        // Group nodes by file → shard
        let mut by_shard: HashMap<String, Vec<NodeRecordV2>> = HashMap::new();

        for node in nodes {
            let shard_name = self.shard_planner.shard_for_file(&node.file);
            by_shard.entry(shard_name).or_default().push(node.clone());
        }

        // Route to each shard's write buffer
        for (shard_name, shard_nodes) in by_shard {
            let shard = self.get_or_create_shard(&shard_name)?;
            shard.add_nodes(&shard_nodes)?;
        }

        Ok(())
    }
}
```

### Multi-Shard Queries

```rust
impl GraphEngineV2 {
    fn get_node(&self, id: u128) -> Result<Option<NodeRecordV2>> {
        // Fan-out to all shards (we don't know which shard has this id)
        // Bloom filter makes this efficient: most shards return None immediately
        for shard in self.shards.values() {
            if let Some(node) = shard.get_node(id)? {
                return Ok(Some(node));
            }
        }
        Ok(None)
    }

    fn query_nodes(&self, query: &NodeQuery) -> Result<Vec<NodeRecordV2>> {
        let mut results = Vec::new();

        // If file filter specified → route to specific shard
        if let Some(ref file) = query.file {
            let shard_name = self.shard_planner.shard_for_file(file);
            if let Some(shard) = self.shards.get(&shard_name) {
                results.extend(shard.query_nodes(query)?);
            }
            return Ok(results);
        }

        // No file filter → fan-out to all shards
        for shard in self.shards.values() {
            results.extend(shard.query_nodes(query)?);
        }
        Ok(results)
    }

    fn get_outgoing_edges(&self, src: u128, edge_types: Option<&[String]>) -> Result<Vec<EdgeRecordV2>> {
        // Edges could be in ANY shard (cross-file edges)
        // Fan-out with bloom filter
        let mut results = Vec::new();
        for shard in self.shards.values() {
            results.extend(shard.get_outgoing_edges(src, edge_types)?);
        }
        Ok(results)
    }
}
```

**Key optimization:** `query_nodes` with `file` filter → direct routing to one shard (no fan-out). This is the common case in incremental re-analysis.

### Multi-Shard Flush

```rust
impl GraphEngineV2 {
    fn flush(&mut self) -> Result<()> {
        let mut all_new_descriptors = Vec::new();

        // Flush each shard with data
        for shard in self.shards.values_mut() {
            if !shard.write_buffer_empty() {
                let descriptors = shard.flush()?;
                all_new_descriptors.extend(descriptors);
            }
        }

        if !all_new_descriptors.is_empty() {
            // Collect all segments: existing (from current manifest) + new
            let all_node_segs = /* merge current + new node segments */;
            let all_edge_segs = /* merge current + new edge segments */;
            let manifest = self.manifest_store.create_manifest(all_node_segs, all_edge_segs, None)?;
            self.manifest_store.commit(manifest.version)?;
        }

        Ok(())
    }
}
```

**Note:** Flush creates ONE manifest for ALL shards. Manifest is the single atomic unit.

### Parallel Shard Writes (rayon)

```rust
use rayon::prelude::*;

fn flush_parallel(&mut self) -> Result<()> {
    // Collect shards with pending data
    let shard_refs: Vec<&mut Shard> = self.shards.values_mut()
        .filter(|s| !s.write_buffer_empty())
        .collect();

    // Parallel flush (each shard writes to its own directory — no conflicts)
    let descriptors: Vec<SegmentDescriptor> = shard_refs.par_iter_mut()
        .map(|shard| shard.flush())
        .collect::<Result<Vec<_>>>()?
        .into_iter()
        .flatten()
        .collect();

    // Sequential manifest update (single atomic operation)
    if !descriptors.is_empty() {
        let manifest = self.build_updated_manifest(descriptors)?;
        self.manifest_store.commit(manifest.version)?;
    }

    Ok(())
}
```

**Why safe:** Each shard writes to its own directory. No file conflicts. Only manifest update is serialized.

---

## Critical Nuances

### 1. Cross-Shard Edges

Edges connect nodes from different files (IMPORTS_FROM, CALLS, etc.). An edge with `src` in shard A and `dst` in shard B → where is the edge stored?

**Decision: Edge stored in src's shard.** Rationale:
- Outgoing edge queries (most common) need only one shard
- Incoming edge queries (less common, used for blast radius) fan-out with dst bloom filter
- Enrichment edges: stored in enrichment shard (I2), not in source file's shard

### 2. Cross-Shard Point Lookup

`get_node(id)` doesn't know which shard contains the node. Must fan-out.

**With bloom filters:** Each shard's node segments have bloom filter. Fan-out is: check N bloom filters → 1 actual scan. For 10 shards = 10 bloom checks (~1µs total) + 1 segment scan.

**Future optimization (T6.1):** Global index maps node_id → shard. O(1) routing.

### 3. Shard Creation on Demand

New file in new directory → new shard. Don't pre-create shards for directories without files.

```rust
fn get_or_create_shard(&mut self, shard_name: &str) -> Result<&mut Shard> {
    if !self.shards.contains_key(shard_name) {
        let shard_path = self.db_path.join("shards").join(shard_name);
        std::fs::create_dir_all(&shard_path)?;
        self.shards.insert(shard_name.to_string(), Shard::new(shard_path));
    }
    Ok(self.shards.get_mut(shard_name).unwrap())
}
```

### 4. Edge Ownership for Analysis vs Enrichment

T2.3 handles analysis edges only. Enrichment edges use composite file context (`__enrichment__/{enricher}/{file}`) — this is T3.1/T5.1 territory.

For T2.3:
- Edges from analysis → stored in src node's shard
- No special enrichment shard logic yet

### 5. Empty Shards

If all files in a directory are deleted, shard becomes empty. On GC, empty shards can be removed (directory deleted). But shard planner should handle this gracefully — don't crash on empty shard.

### 6. count_by_type with Multiple Shards

```rust
fn count_nodes_by_type(&self) -> HashMap<String, u64> {
    let mut counts = HashMap::new();
    for shard in self.shards.values() {
        for (node_type, count) in shard.count_nodes_by_type() {
            *counts.entry(node_type).or_default() += count;
        }
    }
    counts
}
```

Aggregation across shards. Zone maps can provide this without scanning: each segment's zone map lists distinct types, combined with record_count.

---

## Test Plan

### Shard Planner

1. `plan_single_directory` — all files in one dir → one shard
2. `plan_multiple_directories` — files in 3 dirs → 3 shards
3. `plan_nested_directories` — deep nesting → correct shard assignment
4. `plan_small_directory_merged` — dir with 1 file + min_files=3 → merged with parent
5. `plan_deterministic` — same files in different order → same plan
6. `plan_stability` — add new file → existing assignments unchanged
7. `plan_completeness` — every file in exactly one shard

### Multi-Shard Write + Read

8. `write_routed_to_correct_shard` — add nodes with different files → each in correct shard
9. `query_all_shards` — query without file filter → results from all shards
10. `query_specific_shard` — query with file filter → only that shard queried
11. `point_lookup_cross_shard` — get_node finds node regardless of shard

### Cross-Shard Edges

12. `cross_shard_outgoing` — edge src in shard A → get_outgoing from shard A → found
13. `cross_shard_incoming` — edge dst in shard B → get_incoming fans out → found

### Parallel Writes

14. `parallel_flush_correct` — parallel flush of N shards → all data persisted
15. `parallel_flush_manifest` — parallel flush → single manifest with all segments
16. `parallel_result_equals_sequential` — same data, parallel vs sequential flush → identical

### Equivalence

17. `equivalence_single_vs_multi_shard` — same data in single shard vs multi shard → identical query results
18. `equivalence_v1_vs_v2_multi` — same data in v1 engine vs v2 multi-shard → identical results

### Edge Cases

19. `empty_shard` — shard with no data → queries return empty, no crash
20. `single_file_project` — 1 file → 1 shard → works correctly

---

## Performance Targets

| Operation | Target | Notes |
|-----------|--------|-------|
| Shard plan | <1ms for 10K files | Simple directory grouping |
| Point lookup (10 shards) | <100µs | 10 bloom checks + 1 scan |
| File-scoped query | Same as single shard | Direct routing, no fan-out |
| Parallel flush (4 shards) | ~4x single shard throughput | rayon, 4 threads |

---

## Dependencies (Cargo)

```toml
rayon = "1.8"  # Already in project for parallel operations
```
