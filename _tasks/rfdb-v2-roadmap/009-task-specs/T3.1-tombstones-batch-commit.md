# T3.1: Tombstones + Batch Commit (Track 1, Rust)

> Milestone: M3 (Incremental Core)
> Dependencies: T2.1 (Manifest), T2.2 (Single Shard)
> Estimated: ~600 LOC, ~35 tests
> Related docs: [002-roadmap.md](../002-roadmap.md) §2 (Batch Commit), [004-expert-concerns.md](../004-expert-concerns.md) C3/I2/I4/N7, [005-orchestrator-design.md](../005-orchestrator-design.md) §3

---

## High-Level Context

T2.2 даёт write + read. Но пока нет **delete** и нет **atomic multi-file update**. Для инкрементальности нужно:

1. **Tombstones** — пометить удалённые node/edge IDs без перезаписи сегмента
2. **Batch commit** — BeginBatch → AddNodes/AddEdges → CommitBatch = atomic update with delta
3. **File grouping** — RFDB сам группирует nodes по `file` field → tombstone old nodes для тех файлов
4. **CommitDelta** — ответ CommitBatch содержит что изменилось (changedFiles, changedTypes, removedNodeIds)

Это **ядро инкрементального обновления**. После T3.1 RFDB может:
- Принять re-analysis одного файла → tombstone старые данные → записать новые → atomic swap
- Вернуть delta → client/orchestrator знает что изменилось → selective re-enrichment

---

## Existing Code to Study

| File | What to learn |
|------|--------------|
| `src/graph/engine.rs` (lines 700-800) | v1 delta log: `Delta::DeleteNode`, `Delta::DeleteEdge` |
| `src/graph/engine.rs` (lines 1073-1265) | v1 flush: merge delta + segment, handle deleted_segment_ids |
| `src/storage_v2/shard.rs` | Shard from T2.2 |
| `src/storage_v2/manifest.rs` | ManifestStore from T2.1 |
| `src/storage_v2/segment.rs` | NodeSegmentV2 from T1.1 |

---

## New & Modified Files

```
src/storage_v2/
  tombstone.rs            ← NEW: TombstoneSet, tombstone segment format
  batch.rs                ← NEW: BatchState, CommitBatch logic, CommitDelta
src/graph/
  engine_v2.rs            ← MODIFY: add batch protocol, tombstone-aware queries
```

---

## Tombstone Design

### What is a Tombstone?

A tombstone marks a node or edge as deleted WITHOUT modifying the original segment. Immutable segments stay immutable.

```rust
/// Set of deleted IDs per file.
/// When reading, skip records whose id is in the tombstone set.
pub struct TombstoneSet {
    /// Deleted node IDs
    deleted_nodes: HashSet<u128>,

    /// Deleted edge keys (src, dst, edge_type hash)
    deleted_edges: HashSet<(u128, u128, u64)>,  // u64 = hash of edge_type string
}
```

### Tombstone Segment (on disk)

```
Header (16 bytes):
  magic: [u8; 4] = b"TOMB"
  version: u16 = 1
  node_count: u32
  edge_count: u32
  padding: u16

Body:
  [deleted_node_ids: u128 × node_count]
  [deleted_edge_keys: (u128, u128, u64) × edge_count]   // 40 bytes each
```

**Why separate file, not per-segment?** Tombstones are per-shard (or per-file-context), not per-segment. One tombstone set can mark deletions across multiple segments.

### Query Path with Tombstones

```rust
impl Shard {
    fn get_node(&self, id: u128) -> Result<Option<NodeRecordV2>> {
        // Check tombstones first
        if self.tombstones.is_deleted_node(id) {
            return Ok(None);
        }

        // Check write buffer
        if let Some(node) = self.write_buffer.get_node(id) {
            return Ok(Some(node));
        }

        // Check segments (with tombstone filtering)
        for segment in self.node_segments.iter().rev() {
            if segment.maybe_contains(id) {
                // Scan for id
                for i in 0..segment.record_count() {
                    if segment.get_id(i) == id {
                        return Ok(Some(segment.get_record(i)));
                    }
                }
            }
        }
        Ok(None)
    }
}
```

**Nuance:** Tombstone check is O(1) HashSet lookup. Happens BEFORE segment scan.

---

## Batch Commit Protocol

### Wire Protocol Extensions

```rust
// New Request variants (added to existing enum)
BeginBatch,
CommitBatch { tags: Option<HashMap<String, String>> },
AbortBatch,
```

### Batch State

```rust
/// Server-side batch state per connection.
pub struct BatchState {
    /// Is a batch currently open?
    active: bool,

    /// Accumulated nodes (not yet committed)
    pending_nodes: Vec<NodeRecordV2>,

    /// Accumulated edges
    pending_edges: Vec<EdgeRecordV2>,
}

impl BatchState {
    pub fn begin(&mut self) -> Result<()> {
        if self.active { return Err("Batch already active"); }
        self.active = true;
        Ok(())
    }

    pub fn add_nodes(&mut self, nodes: Vec<NodeRecordV2>) {
        self.pending_nodes.extend(nodes);
    }

    pub fn add_edges(&mut self, edges: Vec<EdgeRecordV2>) {
        self.pending_edges.extend(edges);
    }

    pub fn commit(&mut self) -> (Vec<NodeRecordV2>, Vec<EdgeRecordV2>) {
        self.active = false;
        let nodes = std::mem::take(&mut self.pending_nodes);
        let edges = std::mem::take(&mut self.pending_edges);
        (nodes, edges)
    }

    pub fn abort(&mut self) {
        self.active = false;
        self.pending_nodes.clear();
        self.pending_edges.clear();
    }
}
```

### CommitBatch Logic (in engine_v2.rs)

**Note:** `BatchState` lives in `ConnectionState` (per-connection), NOT in `GraphEngineV2`.
The engine's `commit_batch()` takes batch data as parameters.

```rust
/// Called by protocol handler, which owns ConnectionState with BatchState.
/// Engine receives the accumulated data, not internal batch state.
fn commit_batch(
    &mut self,
    nodes: Vec<NodeRecordV2>,
    edges: Vec<EdgeRecordV2>,
    tags: Option<HashMap<String, String>>,
) -> Result<CommitDelta> {
    // 0. Empty batch → no-op
    if nodes.is_empty() && edges.is_empty() {
        return Ok(CommitDelta::empty());
    }

    // 1. Group nodes by file
    let nodes_by_file: HashMap<String, Vec<NodeRecordV2>> = group_by_file(&nodes);
    let changed_files: Vec<String> = nodes_by_file.keys().cloned().collect();

    // 2. For each file in batch: find existing nodes + edges to tombstone
    let mut tombstone_nodes: HashSet<u128> = HashSet::new();
    let mut tombstone_edges: HashSet<(u128, u128, u64)> = HashSet::new();
    let mut old_nodes_by_id: HashMap<u128, NodeRecordV2> = HashMap::new();
    let mut old_edge_types: HashSet<String> = HashSet::new();

    for file in &changed_files {
        // Query all existing nodes for this file (zone-map-pruned)
        let existing = self.query_nodes_by_file(file)?;
        for node in &existing {
            tombstone_nodes.insert(node.id);
            old_nodes_by_id.insert(node.id, node.clone());
        }

        // Query edges owned by this file — bloom-assisted algorithm:
        // 1. Collect all node IDs for this file (already have them above)
        // 2. For each edge segment: check src bloom for any of these IDs
        // 3. Only scan segments where bloom says "maybe"
        // 4. In matching segments: filter edges where src ∈ file_node_ids
        let file_node_ids: HashSet<u128> = existing.iter().map(|n| n.id).collect();
        let existing_edges = self.query_edges_by_src_ids(&file_node_ids)?;
        for edge in &existing_edges {
            old_edge_types.insert(edge.edge_type.clone());
            let key = (edge.src, edge.dst, hash_edge_type(&edge.edge_type));
            tombstone_edges.insert(key);
        }
    }

    // 3. Compute delta BEFORE applying changes
    let delta = compute_delta(
        &old_nodes_by_id, &nodes, &edges,
        &old_edge_types, &changed_files,
    );

    // 4. Apply tombstones
    for file in &changed_files {
        let shard_name = self.shard_planner.shard_for_file(file);
        let shard = self.get_shard_mut(&shard_name)?;
        shard.add_tombstones(&tombstone_nodes, &tombstone_edges);
    }

    // 5. Write new nodes + edges to shards
    self.add_nodes_internal(&nodes)?;
    self.add_edges_internal(&edges)?;

    // 6. Flush all dirty shards + create manifest
    self.flush_with_tags(tags)?;

    Ok(delta)
}

/// Bloom-assisted edge query by src node IDs.
/// For each edge segment: check src bloom filter against file_node_ids.
/// Only scan segments where bloom says "maybe contains".
/// Complexity: O(edge_segments * bloom_check) + O(matching_segments * records)
fn query_edges_by_src_ids(&self, src_ids: &HashSet<u128>) -> Result<Vec<EdgeRecordV2>> {
    let mut results = Vec::new();
    for shard in self.shards.values() {
        for segment in &shard.edge_segments {
            // Bloom filter check: does this segment maybe contain edges from any of our src IDs?
            let bloom_hit = src_ids.iter().any(|id| segment.src_bloom().maybe_contains(*id));
            if !bloom_hit { continue; }

            // Scan segment, filter by src ∈ src_ids
            for i in 0..segment.record_count() {
                let src = segment.get_src(i);
                if src_ids.contains(&src) {
                    results.push(segment.get_record(i));
                }
            }
        }
    }
    Ok(results)
}
```

### Auto-Commit (Backward Compat)

AddNodes without BeginBatch → implicit single-operation batch:

```rust
fn add_nodes(&mut self, nodes: &[NodeRecordV2]) -> Result<()> {
    if self.batch_state.active {
        // In batch mode: accumulate
        self.batch_state.add_nodes(nodes.to_vec());
        Ok(())
    } else {
        // Not in batch: direct write to shard (v1 behavior)
        self.add_nodes_internal(nodes)
    }
}
```

---

## CommitDelta

```rust
#[derive(Debug, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct CommitDelta {
    /// Files that had nodes added/removed/modified
    pub changed_files: Vec<String>,

    /// Total nodes added in this commit
    pub nodes_added: u64,

    /// Total nodes removed (tombstoned)
    pub nodes_removed: u64,

    /// Nodes modified: same semantic_id, different content_hash (I4)
    pub nodes_modified: u64,

    /// IDs of removed nodes (for downstream cleanup)
    pub removed_node_ids: Vec<String>,  // semantic_id strings

    /// Node types present in changed nodes
    pub changed_node_types: Vec<String>,

    /// Edge types present in changed edges
    pub changed_edge_types: Vec<String>,
}
```

### Delta Computation

```rust
fn compute_delta(
    old_nodes: &HashMap<u128, NodeRecordV2>,
    new_nodes: &[NodeRecordV2],
    new_edges: &[EdgeRecordV2],
    old_edge_types: &HashSet<String>,
    changed_files: &[String],
) -> CommitDelta {
    let new_ids: HashSet<u128> = new_nodes.iter().map(|n| n.id).collect();
    let old_ids: HashSet<u128> = old_nodes.keys().cloned().collect();

    // Removed: in old but not in new
    let removed_ids: Vec<u128> = old_ids.difference(&new_ids).cloned().collect();

    // Added: in new but not in old
    let added_ids: Vec<u128> = new_ids.difference(&old_ids).cloned().collect();

    // Modified: in both, but different content_hash (I4)
    let mut modified_count = 0u64;
    for node in new_nodes {
        if let Some(old) = old_nodes.get(&node.id) {
            if old.content_hash != node.content_hash && node.content_hash != 0 {
                modified_count += 1;
            }
        }
    }

    // Collect node types from added/removed/modified nodes
    let mut changed_node_types: HashSet<String> = HashSet::new();
    for node in new_nodes {
        changed_node_types.insert(node.node_type.clone());
    }
    for id in &removed_ids {
        if let Some(old) = old_nodes.get(id) {
            changed_node_types.insert(old.node_type.clone());
        }
    }

    // Collect edge types from BOTH new edges and old tombstoned edges
    let mut changed_edge_types: HashSet<String> = old_edge_types.clone();
    for edge in new_edges {
        changed_edge_types.insert(edge.edge_type.clone());
    }

    CommitDelta {
        changed_files: changed_files.to_vec(),
        nodes_added: added_ids.len() as u64,
        nodes_removed: removed_ids.len() as u64,
        nodes_modified: modified_count,
        removed_node_ids: removed_ids.iter()
            .filter_map(|id| old_nodes.get(id).map(|n| n.semantic_id.clone()))
            .collect(),
        changed_node_types: changed_node_types.into_iter().collect(),
        changed_edge_types: changed_edge_types.into_iter().collect(),
    }
}
```

---

## Critical Nuances

### 1. Edge Ownership = File Context of CommitBatch (I2)

When CommitBatch tombstones a file's data, it must tombstone:
- All nodes with `file == X`
- All edges whose **src** node has `file == X`

**Not dst-based:** Edge belongs to the file that CREATES it (analysis side = src file). If file A imports from file B, the IMPORTS_FROM edge is owned by file A (src=import_node_in_A).

**Why not read src node's file?** That would require read-during-write (C3). Instead, CommitBatch knows which files are being committed → edges for those files are tombstoned.

### 2. Enrichment File Context (I2)

Enrichment edges use composite file context: `__enrichment__/{enricher}/{source_file}`.

This is NOT handled in T3.1. T3.1 handles analysis edges only (file = real file path). T5.1 extends CommitBatch for enrichment contexts.

For T3.1, the `file` field in CommitBatch = real file path only.

### 3. content_hash for Modified Detection (I4)

A node is "modified" (not just added/removed) when:
- Same `semantic_id` (same u128 hash) exists before and after
- But `content_hash` differs

This enables precision diff: "function body changed but function still exists with same name."

**content_hash = 0** means "not computed" (legacy nodes, external modules). Don't count as modified.

### 4. Idempotency

Re-analyzing the same file with identical content → CommitBatch should produce:
- nodes_added = 0
- nodes_removed = 0
- nodes_modified = 0

Because: old nodes have same semantic_ids and same content_hashes as new nodes.

**Implementation:** Tombstone all old, add all new. But delta correctly reports 0 changes because ids match.

**Optimization opportunity (not T3.1):** Skip tombstone + re-write if all content_hashes match. "Nothing changed" fast path.

### 5. Snapshot Isolation (C3)

Pre-commit: readers see previous manifest. During commit: new manifest not yet committed. Post-commit: readers see new manifest.

This is already handled by manifest chain (T2.1): `current.json` atomic pointer. Readers load current manifest at connection time and use that snapshot throughout.

### 6. CommitBatch delta vs DiffSnapshots

Both provide "what changed" but at different levels:
- **CommitDelta** = node-level: specific IDs added/removed/modified, types changed
- **DiffSnapshots** = segment-level: segments added/removed between two manifests

`CommitDelta` is computed DURING commit (has access to old and new nodes). `DiffSnapshots` is computed AFTER (only sees segment lists).

Both should be consistent: `DiffSnapshots(prev, current)` should show the segments affected by the commit.

### 7. ISSUE Node Lifecycle

ISSUE nodes are created by VALIDATION plugins. When file is re-analyzed:
- Old ISSUE nodes for that file → tombstoned (they're associated with old analysis)
- New ISSUE nodes → created by validation (if issues still present)

This is handled naturally by "tombstone all nodes for file" — ISSUE nodes have `file` field too.

### 8. Tombstone Accumulation

Each commit adds tombstones. Over many commits, tombstone set grows. Compaction (T6.1) resolves this: merge segments + apply tombstones → clean segments, tombstones cleared.

Before compaction, tombstone check is O(1) per query (HashSet), but set size grows. For 100K tombstoned IDs: ~1.6MB in memory. Acceptable.

---

## Test Plan

### Tombstones

1. `tombstone_hides_node` — tombstone id → get_node returns None
2. `tombstone_hides_edge` — tombstone edge → get_outgoing excludes it
3. `tombstone_query_excludes` — tombstone nodes → query_nodes skips them
4. `tombstone_persisted` — tombstone → flush → reload → still hidden

### Batch Protocol

5. `begin_commit_basic` — begin → add nodes → commit → data visible
6. `begin_abort` — begin → add nodes → abort → data NOT visible
7. `batch_not_visible_before_commit` — begin → add → query (outside batch) → NOT visible
8. `double_begin_error` — begin → begin → error
9. `commit_without_begin_error` — commit without begin → error

### File Grouping

10. `commit_groups_by_file` — add nodes for 3 files → each in correct shard
11. `commit_tombstones_old_file_data` — re-analyze file → old nodes gone, new present
12. `commit_other_files_unaffected` — commit file A → file B data unchanged

### CommitDelta

13. `delta_added_count` — add new file → nodes_added = N, removed = 0
14. `delta_removed_count` — commit empty file (previously had nodes) → removed = N
15. `delta_modified_count` — same semantic_id, different content_hash → nodes_modified counted
16. `delta_changed_files` — commit 2 files → changedFiles = [file1, file2]
17. `delta_changed_types` — commit FUNCTION + CALL nodes → changedNodeTypes contains both
18. `delta_removed_node_ids` — removed nodes → semantic_id strings in delta
19. `delta_matches_diff` — CommitDelta consistent with DiffSnapshots for same commit

### Idempotency

20. `idempotent_reanalysis` — same content twice → delta shows 0 changes
21. `idempotent_graph_unchanged` — re-analyze same → query results identical before and after

### Auto-Commit (Backward Compat)

22. `add_nodes_without_batch` — direct addNodes → data immediately visible
23. `add_edges_without_batch` — direct addEdges → data visible
24. `auto_commit_produces_no_delta` — no delta returned for auto-commit (v1 compat)

### Atomicity

25. `batch_all_or_nothing` — add 10 nodes → commit → all visible
26. `abort_discards_everything` — add 10 nodes → abort → none visible
27. `concurrent_read_during_batch` — batch open → other reader sees previous snapshot

### Modified Detection (I4)

28. `content_hash_detects_modification` — same id, different hash → modified
29. `content_hash_zero_not_modified` — hash=0 → not counted as modified
30. `content_hash_same_not_modified` — same id, same hash → not modified

### Edge Ownership

31. `edges_tombstoned_by_src_file` — edge src in file A → commit file A → edge tombstoned
32. `edges_not_tombstoned_by_dst_file` — edge dst in file A, src in file B → commit file A → edge NOT tombstoned

### Edge Types in Delta

33. `delta_edge_types_from_new_edges` — new edges have types → in changedEdgeTypes
34. `delta_edge_types_from_removed_edges` — removed edges' types → in changedEdgeTypes

### Benchmark

35. `bench_re_analysis_one_file` — 10K nodes total, re-analyze 1 file (100 nodes) → measure time (should be O(100), not O(10K))

---

## Performance Targets

| Operation | Target | Notes |
|-----------|--------|-------|
| CommitBatch (1 file, 100 nodes) | <10ms | Tombstone + write + manifest |
| CommitBatch (10 files, 1K nodes) | <50ms | Parallel shard writes |
| Delta computation | <1ms | In-memory set operations |
| Tombstone check per query | <100ns | HashSet O(1) |

---

## Dependencies (Cargo)

No new dependencies.
