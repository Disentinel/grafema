# T4.3: Client Streaming (Track 3, TS)

> Milestone: M4 (Integration Gate)
> Dependencies: T4.1 (Wire Protocol v3), T1.3 (Request IDs)
> Estimated: ~250 LOC, ~12 tests
> Related docs: [006-client-spec.md](../006-client-spec.md)

---

## High-Level Context

Большие результаты запросов (>1000 nodes) отправляются чанками. Без streaming один ответ с 50K nodes → 50MB MessagePack → client OOM.

T1.3 (request IDs) — обязательная зависимость: без requestId нельзя понять к какому запросу относится chunk.

---

## Wire Protocol: Chunk Format

```
Server sends:
  { nodes: [...100 items...], done: false, requestId: "r5" }
  { nodes: [...100 items...], done: false, requestId: "r5" }
  { nodes: [...42 items...],  done: true,  requestId: "r5" }
```

Client receives chunks with same `requestId` → accumulates → resolves promise when `done: true`.

---

## Client Implementation

### Streaming Response Parser

```typescript
// Modify _handleResponse to detect chunks
private _handleResponse(response: RFDBResponse): void {
  if (response.requestId && response.done === false) {
    // Streaming chunk — accumulate
    this.accumulateChunk(response.requestId, response);
    return;
  }

  if (response.requestId && response.done === true) {
    // Final chunk — merge and resolve
    this.finalizeStream(response.requestId, response);
    return;
  }

  // Non-streaming response (existing logic)
  // ...
}
```

### Async Generator for Streaming

```typescript
async *queryNodesStream(query?: AttrQuery): AsyncGenerator<WireNode> {
  // Internal: send request, yield chunks as they arrive
  // Uses requestId to correlate chunks
}
```

### Auto-Detection

Server decides whether to stream based on result size. Client doesn't opt-in — it handles both chunked and non-chunked responses transparently.

### Backpressure

Async iteration provides natural backpressure: if consumer is slow, socket buffer fills → TCP backpressure → server pauses.

---

## Critical Nuances

### 1. Chunk Accumulation Buffer

```typescript
private streamBuffers: Map<string, {
  chunks: unknown[];
  resolve: (value: RFDBResponse) => void;
  reject: (error: Error) => void;
}>;
```

Memory management: if stream is abandoned (client timeout), buffer must be cleaned up.

### 2. Stream Abort

Client closes socket or sends abort → server stops sending chunks. Incomplete stream → reject promise with error.

### 3. Non-Streaming Fallback

Small results (<1000 nodes): server sends single response as before. Client handles transparently.

### 4. Timeout Per Stream

Timeout resets with each chunk received (the stream is active, not stuck):
```typescript
// Reset timeout on each chunk
clearTimeout(timer);
timer = setTimeout(() => reject(new Error('Stream timeout')), timeoutMs);
```

---

## Test Plan

1. `small_result_non_streaming` — <100 nodes → single response
2. `large_result_streaming` — >1000 nodes → chunked
3. `stream_accumulation_correct` — all chunks → complete result
4. `stream_equivalence` — streaming result = non-streaming result (same data)
5. `stream_timeout_resets` — slow but active stream → no timeout
6. `stream_abort` — client cancels → partial result error
7. `stream_backpressure` — slow consumer → server waits
8. `async_generator_yields` — queryNodesStream → yields nodes one by one
9. `multiple_concurrent_streams` — two streaming queries → independent (via requestId)
10. `stream_plus_non_stream` — concurrent streaming + point lookup → both correct
11. `empty_stream` — 0 results → done=true immediately
12. `stream_error_mid_chunk` — server error during stream → client gets error
