# T2.2: Single-Shard Read/Write (Track 1, Rust)

> Milestone: M2 (Storage Engine)
> Dependencies: T1.1 (Segment Format), T2.1 (Manifest + Snapshot Chain)
> Estimated: ~2000 LOC, ~28 tests
> Related docs: [002-roadmap.md](../002-roadmap.md) §2 (Batch Commit), [T1.1 spec](./T1.1-segment-format.md), [T2.1 spec](./T2.1-manifest-snapshot-chain.md)

---

## High-Level Context

T1.1 даёт нам иммутабельные сегменты (write once, read many). T2.1 даёт manifest chain для tracking сегментов. T2.2 собирает их вместе в **работающий storage engine для одного шарда**.

**Shard = directory containing segments.** Один шард хранит данные для subset файлов проекта (distribution по файлам придёт в T2.3). Для T2.2 весь граф — один шард.

**Ключевые операции:**
- **Write path:** Принять Vec<NodeRecordV2> → записать в write buffer → flush → новый сегмент + manifest update
- **Point lookup:** Bloom filter check → segment scan → find node by u128 id
- **Attribute search:** Zone map pruning → columnar scan → filter by attributes
- **Neighbors query:** Edge segment scan (bloom on src/dst → candidates → filter)
- **Write buffer:** In-memory accumulation before flush to segment

**Результат T2.2:** `engine_v2.rs` — работающий v2 engine за `GraphEngine` trait (single shard mode).

---

## Existing Code to Study

| File | What to learn |
|------|--------------|
| `src/graph/engine.rs` (~1300 LOC) | v1 engine: HashMap-based delta, flush logic, query patterns |
| `src/graph/mod.rs` | `GraphStore` trait (v1 interface) |
| `src/graph/index_set.rs` | Adjacency lists: `HashMap<u128, Vec<u128>>` for edge traversal |
| `src/storage_v2/segment.rs` | NodeSegmentV2, EdgeSegmentV2 from T1.1 |
| `src/storage_v2/writer.rs` | SegmentWriter from T1.1 |
| `src/storage_v2/manifest.rs` | ManifestStore from T2.1 |

---

## New & Modified Files

```
src/storage_v2/
  shard.rs                ← NEW: Shard struct (segments + write buffer for one shard)
  write_buffer.rs         ← NEW: In-memory buffer for pending writes
src/graph/
  engine_v2.rs            ← NEW: GraphEngineV2 (single-shard mode)
  mod.rs                  ← MODIFY: Add GraphEngine trait, both implementations
```

---

## Architecture Overview

```
                    GraphEngineV2
                         │
                    ┌────┴─────┐
                    │  Shard   │
                    └────┬─────┘
               ┌─────────┼──────────┐
               │         │          │
          WriteBuffer  Segments  ManifestStore
          (in-memory)  (mmap'd)  (JSON files)
               │         │          │
        Vec<NodeV2>  [seg1,seg2]  manifests/
        Vec<EdgeV2>  (immutable)  current.json
```

**Write path:** add_nodes() → WriteBuffer → flush() → SegmentWriter → new segment → ManifestStore.create_manifest() → commit()

**Read path:** query → WriteBuffer (check first) → Segments (bloom + zone map → scan) → merge results

---

## Data Structures

### WriteBuffer

```rust
/// In-memory buffer for pending writes.
/// Flushed to segment when:
/// 1. Explicit flush() call
/// 2. Buffer exceeds size threshold
/// 3. Before read operations that need consistent view (auto-flush)
pub struct WriteBuffer {
    nodes: Vec<NodeRecordV2>,
    edges: Vec<EdgeRecordV2>,

    /// Index for fast point lookup in buffer
    node_index: HashMap<u128, usize>,  // id → position in nodes vec

    /// Src adjacency for edge lookups in buffer
    src_index: HashMap<u128, Vec<usize>>,  // src_id → positions in edges vec

    /// Dst adjacency for edge lookups in buffer
    dst_index: HashMap<u128, Vec<usize>>,  // dst_id → positions in edges vec
}

impl WriteBuffer {
    pub fn new() -> Self;

    pub fn add_nodes(&mut self, nodes: &[NodeRecordV2]);
    pub fn add_edges(&mut self, edges: &[EdgeRecordV2]);

    /// Point lookup by u128 id
    pub fn get_node(&self, id: u128) -> Option<&NodeRecordV2>;

    /// Outgoing edges from src
    pub fn get_outgoing(&self, src: u128) -> Vec<&EdgeRecordV2>;

    /// Incoming edges to dst
    pub fn get_incoming(&self, dst: u128) -> Vec<&EdgeRecordV2>;

    /// Node count in buffer
    pub fn node_count(&self) -> usize;

    /// Edge count in buffer
    pub fn edge_count(&self) -> usize;

    /// Take all records and reset buffer (for flush)
    pub fn drain(&mut self) -> (Vec<NodeRecordV2>, Vec<EdgeRecordV2>);

    /// Is buffer empty?
    pub fn is_empty(&self) -> bool;
}
```

**Nuances:**
- `node_index` enables O(1) point lookup in buffer (before checking segments)
- `src_index`/`dst_index` enable edge traversal including unflushed edges
- `drain()` moves data out (no copy) — Vec ownership transferred to caller

### Shard

```rust
/// A shard = collection of immutable segments + write buffer.
/// Represents a partition of the graph (in T2.2: entire graph = single shard).
pub struct Shard {
    /// Path to shard directory
    path: PathBuf,

    /// Opened node segments (mmap'd)
    node_segments: Vec<NodeSegmentV2>,

    /// Opened edge segments (mmap'd)
    edge_segments: Vec<EdgeSegmentV2>,

    /// In-memory write buffer (unflushed data)
    write_buffer: WriteBuffer,

    /// Next segment ID counter
    next_segment_id: u64,
}

impl Shard {
    /// Open shard from manifest (load described segments)
    pub fn open(path: &Path, manifest: &Manifest) -> Result<Self>;

    /// Add nodes to write buffer
    pub fn add_nodes(&mut self, nodes: &[NodeRecordV2]) -> Result<()>;

    /// Add edges to write buffer
    pub fn add_edges(&mut self, edges: &[EdgeRecordV2]) -> Result<()>;

    /// Flush write buffer to new segment(s). Returns new segment descriptors.
    pub fn flush(&mut self) -> Result<Vec<SegmentDescriptor>>;

    // --- Query operations ---

    /// Point lookup by u128 id
    pub fn get_node(&self, id: u128) -> Result<Option<NodeRecordV2>>;

    /// Attribute search (type, name, file filters)
    pub fn query_nodes(&self, query: &NodeQuery) -> Result<Vec<NodeRecordV2>>;

    /// Outgoing edges from node
    pub fn get_outgoing_edges(&self, src: u128, edge_types: Option<&[String]>) -> Result<Vec<EdgeRecordV2>>;

    /// Incoming edges to node
    pub fn get_incoming_edges(&self, dst: u128, edge_types: Option<&[String]>) -> Result<Vec<EdgeRecordV2>>;

    /// Total node count (segments + buffer)
    pub fn node_count(&self) -> u64;

    /// Total edge count (segments + buffer)
    pub fn edge_count(&self) -> u64;
}
```

### NodeQuery (filter for attribute search)

```rust
#[derive(Debug, Default)]
pub struct NodeQuery {
    pub node_type: Option<String>,
    pub name: Option<String>,
    pub file: Option<String>,
}
```

---

## Query Algorithms

### Point Lookup: `get_node(id: u128)`

```
1. Check write buffer (O(1) HashMap lookup)
   → Found? Return immediately.

2. For each node segment (newest → oldest):
   a. Bloom filter check: maybe_contains(id)?
      → Definitely not? Skip segment.
   b. Linear scan of id column for match
      → Found? Reconstruct full NodeRecordV2, return.

3. Not found in any segment → None
```

**Nuances:**
- Segments checked newest-first (most recent data wins if duplicates)
- Bloom filter eliminates ~99% of segment scans for missing IDs
- Linear scan of u128 column is fast on mmap'd data (sequential read, CPU cache friendly)
- **Optimization (future, T6.1):** After compaction, global index provides O(1) lookup

### Attribute Search: `query_nodes(query)`

```
1. Collect results from write buffer:
   Linear scan of buffer, apply filters → add to result set

2. For each node segment:
   a. Zone map check: does segment contain query.node_type? query.file?
      → No match? Skip entire segment.
   b. Columnar scan with filter:
      - If node_type filter: scan node_type column, collect matching indices
      - If file filter: scan file column, collect matching indices
      - Intersect index sets
      - Reconstruct full records for matching indices

3. Merge buffer results + segment results
   → Dedup by id (buffer wins over segments)
```

**Nuances:**
- Zone map provides free segment-level pruning. For file-scoped shards (T2.3), zone map on `file` is nearly perfect — segment contains exactly 1 file.
- Columnar scan reads only needed columns (not full records). This is the key advantage of columnar format.
- Dedup: if same u128 id appears in buffer AND segment, buffer version wins (it's newer).

### Neighbors: `get_outgoing_edges(src, edge_types)`

```
1. Check write buffer src_index → matching edges

2. For each edge segment:
   a. Src bloom filter check: maybe_contains(src)?
      → No? Skip segment.
   b. Linear scan of src column for matches
   c. If edge_types filter: check edge_type for matching records
   d. Reconstruct matching EdgeRecordV2

3. Merge buffer + segment results
```

### Incoming Edges: `get_incoming_edges(dst, edge_types)`

Same as outgoing but uses **dst bloom filter** (N8) and scans dst column.

---

## Write Path: Flush

```
1. Drain write buffer → (nodes: Vec<NodeRecordV2>, edges: Vec<EdgeRecordV2>)

2. If nodes not empty:
   a. Create SegmentWriter
   b. Add all nodes
   c. Write to segments/seg_{id}_nodes.bin
   d. Open new NodeSegmentV2 (mmap)
   e. Add to shard.node_segments

3. If edges not empty:
   a. Create SegmentWriter for edges
   b. Add all edges
   c. Write to segments/seg_{id}_edges.bin
   d. Open new EdgeSegmentV2 (mmap)
   e. Add to shard.edge_segments

4. Return Vec<SegmentDescriptor> (for manifest update)
```

**Nuances:**
- Flush creates at most 2 new segment files (1 nodes + 1 edges)
- If only nodes added (no edges), only nodes segment created
- Segment ID comes from shard's monotonic counter
- After flush, write buffer is empty — next writes go into fresh buffer

---

## GraphEngineV2

```rust
/// v2 engine implementation. Single-shard mode for T2.2.
/// Implements GraphEngine trait (same interface as v1).
pub struct GraphEngineV2 {
    shard: Shard,
    manifest_store: ManifestStore,
    id_gen: IdGen,  // BLAKE3 string→u128 (from id_gen.rs, shared with v1)
}

impl GraphEngineV2 {
    pub fn create(db_path: &Path) -> Result<Self>;
    pub fn open(db_path: &Path) -> Result<Self>;
    pub fn create_ephemeral() -> Result<Self>;
}

impl GraphEngine for GraphEngineV2 {
    fn add_nodes(&mut self, nodes: &[NodeRecord]) -> Result<()> {
        // Convert NodeRecord → NodeRecordV2 (v1 compat layer)
        let v2_nodes = nodes.iter().map(|n| convert_to_v2(n)).collect();
        self.shard.add_nodes(&v2_nodes)
    }

    fn get_node(&self, id: u128) -> Result<Option<NodeRecord>> {
        // Lookup in shard, convert back to v1 NodeRecord if needed
        self.shard.get_node(id).map(|opt| opt.map(|n| convert_to_v1(&n)))
    }

    fn query_nodes(&self, query: &NodeQuery) -> Result<Vec<NodeRecord>> {
        self.shard.query_nodes(query).map(|nodes| nodes.into_iter().map(|n| convert_to_v1(&n)).collect())
    }

    fn get_outgoing_edges(&self, src: u128) -> Result<Vec<EdgeRecord>> {
        self.shard.get_outgoing_edges(src, None).map(|edges| edges.into_iter().map(|e| convert_edge_to_v1(&e)).collect())
    }

    fn flush(&mut self) -> Result<()> {
        let new_descriptors = self.shard.flush()?;
        if !new_descriptors.is_empty() {
            // Collect all segment descriptors (existing + new)
            let all_node_segs = /* current manifest nodes + new node segs */;
            let all_edge_segs = /* current manifest edges + new edge segs */;
            let manifest = self.manifest_store.create_manifest(all_node_segs, all_edge_segs, None)?;
            self.manifest_store.commit(manifest.version)?;
        }
        Ok(())
    }

    // ... etc for all GraphEngine methods
}
```

---

## Critical Nuances

### 1. NodeRecord v1 ↔ NodeRecordV2 Conversion

v1 `NodeRecord` has `type_id: Option<String>`, `deleted: bool`, `exported: bool`, `version: String`.
v2 `NodeRecordV2` has `node_type: String` (required), `content_hash: u64`, no deleted/exported/version.

Conversion rules:
- `type_id: None` → `node_type: "UNKNOWN"` (shouldn't happen in practice)
- `exported` → move to metadata JSON
- `deleted` → tombstone (handled in T3.1, not here)
- `version` → dropped (snapshot numbers replace per-node versions)
- `metadata.originalId` → dropped (semantic_id IS the identity now)
- `content_hash` → `0` for v1 nodes (no hash available)

### 2. Write Buffer vs Segments: Consistency

Read operations must see BOTH write buffer AND segments. This is the "union" semantic:
- Buffer = latest writes (not yet flushed)
- Segments = previously flushed data
- Result = buffer ∪ segments, with buffer winning on id conflicts

**No auto-flush before reads.** Auto-flush would be expensive and unpredictable. Instead, buffer is always queried alongside segments.

### 3. Segment Accumulation

Each flush adds a new segment. After 10 flushes → 10 segments per type. Query performance degrades with more segments (each must be scanned).

**T2.2 doesn't solve this.** Compaction (T6.1) merges segments. For now, performance is acceptable with <50 segments (bloom filters skip most).

### 4. Segment Order for Reads

Newest segments first. If same node exists in segment 10 and segment 5, segment 10's version wins. This is the "last write wins" semantic.

**In T2.2, this shouldn't happen** — no tombstones, no updates. A node appears in exactly one segment. But the ordering is needed for T3.1 (tombstones).

### 5. Edge Deduplication

v1 uses `edge_keys: HashSet<(u128, u128, String)>` for dedup. v2 write buffer should do the same — prevent duplicate edges within buffer.

```rust
impl WriteBuffer {
    fn add_edges(&mut self, edges: &[EdgeRecordV2]) {
        for edge in edges {
            let key = (edge.src, edge.dst, edge.edge_type.clone());
            if self.edge_dedup.insert(key) {
                let idx = self.edges.len();
                self.edges.push(edge.clone());
                self.src_index.entry(edge.src).or_default().push(idx);
                self.dst_index.entry(edge.dst).or_default().push(idx);
            }
        }
    }
}
```

### 6. String Resolution at Query Time

v2 stores u128 IDs internally but returns `semantic_id: String` from segments. The string is stored in segment's string table — zero-copy read via mmap.

When returning `NodeRecord` (v1 compat), `id` field = semantic_id string. When the engine is fully v2, wire protocol will use semantic_id directly.

### 7. Ephemeral Databases

Ephemeral databases = write buffer only, no segments, no manifest. All data in memory. Flush = no-op (or: flush to in-memory segment for query consistency).

```rust
if self.is_ephemeral() {
    // Keep data in write buffer permanently
    // Or: flush to in-memory Vec<u8> segment (no disk I/O)
    return Ok(());
}
```

### 8. Existing Queries to Support

v1 has many query types that T2.2 must support:

| v1 Method | v2 Implementation |
|-----------|-------------------|
| `get_node(id)` | Bloom → scan (described above) |
| `node_exists(id)` | Same as get_node but returns bool |
| `find_by_type(type)` | Zone map prune → type column scan |
| `find_by_attr(query)` | Zone map prune → multi-column scan |
| `neighbors(id, types)` | Src bloom → edge scan → collect dst |
| `bfs/dfs/reachability` | Iterative neighbor queries (existing graph algorithm code) |
| `get_outgoing_edges(id)` | Src bloom → edge scan |
| `get_incoming_edges(id)` | Dst bloom → edge scan |
| `node_count()` | Sum of segment counts + buffer count |
| `edge_count()` | Sum of segment counts + buffer count |
| `count_by_type(types)` | Full scan or zone map shortcut |

---

## Test Plan

### Write + Read Roundtrip

1. `add_nodes_then_query` — add 10 nodes → query all → 10 results
2. `add_edges_then_neighbors` — add edges → get_outgoing → correct neighbors
3. `get_node_by_id` — add node → get by u128 → found
4. `get_node_not_found` — get nonexistent id → None
5. `node_exists_true` — add → exists → true
6. `node_exists_false` — not added → exists → false

### Write Buffer

7. `buffer_query_unflushed` — add to buffer (no flush) → queryable
8. `buffer_point_lookup` — add to buffer → get_node → found
9. `buffer_edge_traversal` — add edges to buffer → get_outgoing → found
10. `buffer_dedup_edges` — add same edge twice → count = 1

### Flush + Segments

11. `flush_creates_segment` — add + flush → segment file exists
12. `flushed_data_queryable` — flush → query → data found
13. `multiple_flushes` — flush 3 times → 3 segment pairs, all queryable
14. `flush_updates_manifest` — flush → manifest version increments

### Mixed Buffer + Segments

15. `buffer_plus_segment_query` — flush some, add more (unflushed) → both visible in query
16. `buffer_wins_on_conflict` — same id in buffer and segment → buffer version returned

### Attribute Search

17. `query_by_type` — add FUNCTION + CALL nodes → query type=FUNCTION → only functions
18. `query_by_file` — nodes in different files → query file=X → correct subset
19. `zone_map_skip` — segment has no FUNCTION nodes → zone map skips it (verify via logs or stats)

### Equivalence with v1

20. `equivalence_add_get` — same data in v1 HashMap engine and v2 shard → identical get_node results
21. `equivalence_query_by_type` — same data → identical query results
22. `equivalence_neighbors` — same data → identical neighbor results
23. `equivalence_edge_types_filter` — same data → identical filtered edge results

### Performance

24. `bench_point_lookup` — 10K nodes → random lookups → measure latency vs v1
25. `bench_query_by_type` — 10K nodes → type query → measure latency
26. `bench_write_throughput` — batch add → measure throughput

### Edge Cases

27. `empty_shard` — no data → all queries return empty
28. `single_node` — one node → all queries correct

---

## Performance Targets

| Operation | v2 Target (L0, no compaction) | v1 Baseline |
|-----------|-------------------------------|-------------|
| Point lookup | <50µs (bloom + scan) | <1µs (HashMap) |
| Attribute search | <5ms for 10K nodes | <5ms |
| Neighbor query | <100µs per node | <10µs (adjacency list) |
| Write (buffer) | >1M nodes/sec | >1M nodes/sec |
| Flush | >500K nodes/sec to disk | N/A |

**Note:** v2 L0 (pre-compaction) is expected to be slower than v1 for point lookups (bloom + scan vs HashMap). After compaction (T6.1) with global index, target is <10µs. The 2x threshold in milestones doc applies to L0.

---

## Dependencies (Cargo)

No new dependencies beyond T1.1 and T2.1.
