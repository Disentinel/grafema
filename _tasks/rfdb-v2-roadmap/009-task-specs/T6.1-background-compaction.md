# T6.1: Background Compaction (Track 1, Rust)

> Milestone: M6 (Performance)
> Dependencies: T4.1 (Working v2 engine)
> Estimated: ~1500 LOC, ~25 tests
> Related docs: [002-roadmap.md](../002-roadmap.md), [004-expert-concerns.md](../004-expert-concerns.md) I3

---

## High-Level Context

After many flush cycles, each shard accumulates L0 segments. Queries must scan all segments → degrading performance. Compaction merges L0 segments into L1: sorted, deduplicated, tombstones applied, inverted indexes built.

**Analogy:** LSM-tree compaction. L0 = recent writes (unsorted). L1 = merged and indexed.

---

## Design

### Compaction Trigger

```rust
if shard.l0_segment_count() > L0_THRESHOLD {  // default: 10
    schedule_compaction(shard);
}
```

### Merge Process

1. Read all L0 node segments → merge into sorted Vec<NodeRecordV2> (by u128 id)
2. Apply tombstones: skip deleted IDs
3. Dedup: if same id in multiple segments → keep newest
4. Write single L1 node segment (sorted)
5. Build inverted indexes during write
6. Same for edge segments
7. Create new manifest: L0 segments replaced by L1 + indexes
8. Atomic swap (manifest commit)
9. Old L0 segments → GC

### Inverted Index

Built during compaction (free — already iterating all records):

```rust
/// by_type: "FUNCTION" → [offset_0, offset_5, offset_23, ...]
/// by_name: "processData" → [offset_5]
/// by_file: "src/app.js" → [offset_0, offset_1, ..., offset_50]
pub struct InvertedIndex {
    by_type: HashMap<String, Vec<u32>>,   // type → offsets in L1 segment
    by_name: HashMap<String, Vec<u32>>,
    by_file: HashMap<String, Vec<u32>>,
}
```

Stored as separate `.idx` file alongside L1 segment.

### Global Index

```rust
/// Sorted array: (u128 node_id, shard_name, segment_id, offset)
/// Binary search for O(log n) point lookup.
/// Built after compaction across all shards.
pub struct GlobalIndex {
    entries: Vec<GlobalIndexEntry>,  // Sorted by node_id
}

impl GlobalIndex {
    pub fn lookup(&self, id: u128) -> Option<(String, u64, u32)> {
        // Binary search
    }
}
```

### Blue/Green Compaction

1. Build L1 segment + indexes in temp location
2. Verify correctness (spot-check queries)
3. Atomic manifest swap (L0 → L1)
4. Old L0 segments → GC

No reader disruption: readers use old segments until manifest swap.

---

## Critical Nuances

### 1. Concurrent Reads During Compaction

Compaction reads L0 segments while queries also read them. mmap ensures safe concurrent reads. New L1 segment is written to a new file — no in-place modification.

### 2. Query Equivalence

**Invariant:** before compaction = after compaction (same query results). This is the primary correctness criterion.

### 3. Tombstone Application

Compaction is the ONLY time tombstones are applied (removed from data). After compaction, tombstone set can be cleared for compacted segments.

### 4. GC Safety

Old L0 segments moved to gc/ only after:
- New manifest committed (L1 referenced)
- No readers still using old manifest (check refcount or use timeout)

---

## Test Plan

1. `compaction_query_equivalence` — before = after compaction
2. `tombstone_applied` — compacted segment has no tombstoned records
3. `inverted_index_correct` — index query = scan query results
4. `global_index_all_nodes` — every node reachable via global index
5. `concurrent_reads_during_compaction` — no torn reads
6. `blue_green_atomic` — compaction failure → old segments still valid
7. `gc_after_compaction` — old L0 segments moved to gc/
8. `sorted_l1` — L1 segment is sorted by node id
9. `dedup_in_l1` — duplicate ids → only newest survives
10-25. Performance benchmarks, edge cases, multi-shard compaction
