# Joel Spolsky — Technical Implementation Plan v2 for RFD-5

**Date:** 2026-02-13
**Task:** T2.1 Manifest + Snapshot Chain (Revised)
**Estimated LOC:** ~750
**Estimated Tests:** ~45
**Status:** Updated after Don's revision addressing Steve Jobs review

---

## Executive Summary

This specification incorporates all 5 fixes from Steve Jobs' architectural review:

1. **ManifestIndex** — O(1) list_snapshots instead of O(M×S)
2. **Tag Index** — O(1) find_snapshot instead of O(N×S)
3. **Derived Paths** — segment_id + shard_id instead of embedded file_path string
4. **Referenced Segments Tracking** — O(F) GC instead of O(R×S + F)
5. **DurabilityMode** — Configurable fsync (Strict vs Relaxed)

**Core pattern:** Delta Lake transaction log + Apache Iceberg manifest index. Immutable manifests + cached index + atomic pointer swap = ACID commits with O(1) metadata operations.

---

## 1. Complete Data Structures

### 1.1 Manifest (Unchanged Core, Updated References)

```rust
/// Manifest: immutable snapshot descriptor.
///
/// Each manifest represents a consistent point-in-time view of the database.
/// Manifests are immutable after commit except for tags (which can be modified
/// atomically via separate write).
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct Manifest {
    /// Manifest version (sequential, monotonic, gaps allowed after crash recovery)
    pub version: u64,

    /// Creation timestamp (Unix epoch seconds)
    pub created_at: u64,

    /// Active node segments in this snapshot
    pub node_segments: Vec<SegmentDescriptor>,

    /// Active edge segments in this snapshot
    pub edge_segments: Vec<SegmentDescriptor>,

    /// Optional tags for snapshot identification.
    /// Empty HashMap = no tags. Common tags:
    /// - "analysis_run": "success" | "failed"
    /// - "commit_sha": git commit hash
    /// - "build_number": CI build identifier
    #[serde(default)]
    pub tags: HashMap<String, String>,

    /// Pre-computed aggregate statistics
    pub stats: ManifestStats,

    /// Previous manifest version (None for first manifest, v1)
    /// Enables chain traversal without directory scanning.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parent_version: Option<u64>,
}
```

**Invariants:**
- `version >= 1` (version 0 reserved for "no database")
- `created_at > 0` (Unix epoch start is 1970-01-01)
- `node_segments` and `edge_segments` can be empty (valid snapshot of empty database)
- `stats` must match sum of segment descriptors (validated in debug builds)

### 1.2 SegmentDescriptor (REVISED: Derived Paths)

```rust
/// Segment type enumeration.
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum SegmentType {
    Nodes,
    Edges,
}

/// Segment descriptor: segment identity + zone map summary.
///
/// Bridges from ephemeral SegmentMeta (returned by writer) to serializable
/// manifest format. Includes zone map data for query planning without opening segments.
///
/// CRITICAL: file_path is DERIVED at runtime, not stored. This enables sharding
/// without manifest rewrites (T2.2).
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub struct SegmentDescriptor {
    /// Unique segment ID (globally monotonic within database)
    /// Generated by ManifestStore::next_segment_id()
    pub segment_id: u64,

    /// Segment type (nodes or edges)
    pub segment_type: SegmentType,

    /// Optional shard ID (None = flat segments/ directory, Some(n) = segments/0n/ directory)
    /// Phase 1: always None. T2.2 adds sharding.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub shard_id: Option<u16>,

    /// Record count (nodes or edges)
    pub record_count: u64,

    /// File size in bytes
    pub byte_size: u64,

    /// Zone map: node types (empty for edge segments)
    /// Example: {"FUNCTION", "CLASS", "http:route"}
    #[serde(default, skip_serializing_if = "HashSet::is_empty")]
    pub node_types: HashSet<String>,

    /// Zone map: file paths (empty for edge segments)
    /// Example: {"src/main.rs", "src/lib.rs"}
    #[serde(default, skip_serializing_if = "HashSet::is_empty")]
    pub file_paths: HashSet<String>,

    /// Zone map: edge types (empty for node segments)
    /// Example: {"CALLS", "IMPORTS_FROM"}
    #[serde(default, skip_serializing_if = "HashSet::is_empty")]
    pub edge_types: HashSet<String>,
}

impl SegmentDescriptor {
    /// Convert SegmentMeta (from writer) to SegmentDescriptor (for manifest).
    ///
    /// Called after segment writer finishes:
    /// ```
    /// let meta = writer.finish(&mut file)?;
    /// let descriptor = SegmentDescriptor::from_meta(
    ///     store.next_segment_id(),
    ///     SegmentType::Nodes,
    ///     None, // shard_id (Phase 1: always None)
    ///     meta,
    /// );
    /// ```
    pub fn from_meta(
        segment_id: u64,
        segment_type: SegmentType,
        shard_id: Option<u16>,
        meta: SegmentMeta,
    ) -> Self {
        Self {
            segment_id,
            segment_type,
            shard_id,
            record_count: meta.record_count,
            byte_size: meta.byte_size,
            node_types: meta.node_types,
            file_paths: meta.file_paths,
            edge_types: meta.edge_types,
        }
    }

    /// Derive file path from segment_id, shard_id, segment_type.
    ///
    /// Path generation logic centralized here (easy to change in T2.2).
    ///
    /// Phase 1: shard_id = None → "segments/seg_000001_nodes.seg"
    /// T2.2: shard_id = Some(5) → "segments/05/seg_000001_nodes.seg"
    pub fn file_path(&self, db_path: &Path) -> PathBuf {
        let type_suffix = match self.segment_type {
            SegmentType::Nodes => "nodes",
            SegmentType::Edges => "edges",
        };

        let filename = format!("seg_{:06}_{}.seg", self.segment_id, type_suffix);

        if let Some(shard_id) = self.shard_id {
            // Sharded: segments/{shard_id:02}/seg_{id}_{type}.seg
            db_path
                .join("segments")
                .join(format!("{:02}", shard_id))
                .join(filename)
        } else {
            // Flat: segments/seg_{id}_{type}.seg
            db_path.join("segments").join(filename)
        }
    }

    /// Derive relative path string for logging/debugging
    pub fn relative_path(&self) -> String {
        let type_suffix = match self.segment_type {
            SegmentType::Nodes => "nodes",
            SegmentType::Edges => "edges",
        };
        let filename = format!("seg_{:06}_{}.seg", self.segment_id, type_suffix);

        if let Some(shard_id) = self.shard_id {
            format!("segments/{:02}/{}", shard_id, filename)
        } else {
            format!("segments/{}", filename)
        }
    }

    /// Check if segment might contain records matching filters.
    /// Returns true if zone maps indicate potential match (false = definite miss).
    pub fn may_contain(
        &self,
        node_type: Option<&str>,
        file_path: Option<&str>,
        edge_type: Option<&str>,
    ) -> bool {
        if let Some(nt) = node_type {
            if !self.node_types.is_empty() && !self.node_types.contains(nt) {
                return false; // definite miss
            }
        }
        if let Some(fp) = file_path {
            if !self.file_paths.is_empty() && !self.file_paths.contains(fp) {
                return false; // definite miss
            }
        }
        if let Some(et) = edge_type {
            if !self.edge_types.is_empty() && !self.edge_types.contains(et) {
                return false; // definite miss
            }
        }
        true // possible match
    }
}
```

**Migration path:**
- Phase 1: `shard_id = None` always (flat directory)
- T2.2: Add sharding config → new segments get `shard_id = Some(hash(segment_id) % num_shards)`
- Old segments with `shard_id = None` still work (derive path correctly)
- No manifest rewrite needed

### 1.3 ManifestStats (Unchanged)

```rust
/// Aggregate statistics for a manifest (sum of all segment descriptors).
///
/// Pre-computed to avoid scanning segment list on every query.
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
pub struct ManifestStats {
    /// Total node count across all node segments
    pub total_nodes: u64,

    /// Total edge count across all edge segments
    pub total_edges: u64,

    /// Number of node segments
    pub node_segment_count: u32,

    /// Number of edge segments
    pub edge_segment_count: u32,
}

impl ManifestStats {
    /// Compute stats from segment descriptors.
    pub fn from_segments(
        node_segments: &[SegmentDescriptor],
        edge_segments: &[SegmentDescriptor],
    ) -> Self {
        Self {
            total_nodes: node_segments.iter().map(|s| s.record_count).sum(),
            total_edges: edge_segments.iter().map(|s| s.record_count).sum(),
            node_segment_count: node_segments.len() as u32,
            edge_segment_count: edge_segments.len() as u32,
        }
    }

    /// Validate stats match segment descriptors (debug builds only).
    #[cfg(debug_assertions)]
    pub fn validate(
        &self,
        node_segments: &[SegmentDescriptor],
        edge_segments: &[SegmentDescriptor],
    ) {
        let expected = Self::from_segments(node_segments, edge_segments);
        debug_assert_eq!(
            self.total_nodes, expected.total_nodes,
            "stats mismatch: total_nodes"
        );
        debug_assert_eq!(
            self.total_edges, expected.total_edges,
            "stats mismatch: total_edges"
        );
        debug_assert_eq!(
            self.node_segment_count, expected.node_segment_count,
            "stats mismatch: node_segment_count"
        );
        debug_assert_eq!(
            self.edge_segment_count, expected.edge_segment_count,
            "stats mismatch: edge_segment_count"
        );
    }
}
```

### 1.4 ManifestIndex (NEW: O(1) Metadata Operations)

```rust
/// ManifestIndex: cached metadata for all snapshots + tag index + GC reference tracking.
///
/// Single-file index that eliminates O(N) operations:
/// - list_snapshots() → O(1) (read index)
/// - find_snapshot() → O(1) (tag index lookup)
/// - gc_collect() → O(F) (scan segments/ dir, check referenced set)
///
/// Updated atomically during commit (written to manifest_index.json.tmp, then renamed).
///
/// Pattern: Apache Iceberg's manifest list file.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct ManifestIndex {
    /// Latest manifest version (redundant with current.json, but convenient)
    pub latest_version: u64,

    /// All snapshot metadata (sorted by version ascending)
    /// Contains version, created_at, tags, stats for every manifest ever created.
    pub snapshots: Vec<SnapshotInfo>,

    /// Tag index: tag_key -> tag_value -> version
    /// Enables O(1) find_snapshot() lookup.
    /// Example: {"commit_sha": {"abc123": 5, "def456": 7}}
    #[serde(default)]
    pub tag_index: HashMap<String, HashMap<String, u64>>,

    /// All segment IDs referenced by ANY active manifest (union across all versions).
    /// Used by GC: segments NOT in this set are unreferenced → safe to collect.
    /// Updated during commit: add new segments, keep all segments from all manifests.
    pub referenced_segments: HashSet<u64>,
}

impl ManifestIndex {
    /// Create empty index (for new database)
    pub fn new() -> Self {
        Self {
            latest_version: 0,
            snapshots: Vec::new(),
            tag_index: HashMap::new(),
            referenced_segments: HashSet::new(),
        }
    }

    /// Add snapshot to index (called during commit)
    ///
    /// Complexity: O(T + S) where T = tags, S = segments
    pub fn add_snapshot(&mut self, manifest: &Manifest) {
        // Add to snapshots list
        self.snapshots.push(SnapshotInfo::from_manifest(manifest));

        // Update tag index
        for (key, value) in &manifest.tags {
            self.tag_index
                .entry(key.clone())
                .or_insert_with(HashMap::new)
                .insert(value.clone(), manifest.version);
        }

        // Add segments to referenced set
        for seg in manifest
            .node_segments
            .iter()
            .chain(manifest.edge_segments.iter())
        {
            self.referenced_segments.insert(seg.segment_id);
        }

        // Update latest version
        self.latest_version = manifest.version;
    }

    /// Remove old snapshot from index (called during manifest GC, not segment GC)
    ///
    /// Note: Phase 1 does NOT implement manifest GC (keep all manifests).
    /// This method included for completeness (T2.2 will use it).
    ///
    /// Complexity: O(M + T) where M = snapshots, T = tag entries
    pub fn remove_snapshot(&mut self, version: u64) {
        // Remove from snapshots list
        self.snapshots.retain(|info| info.version != version);

        // Remove from tag index
        for tag_values in self.tag_index.values_mut() {
            tag_values.retain(|_, v| *v != version);
        }
        // Clean up empty tag keys
        self.tag_index.retain(|_, values| !values.is_empty());

        // Note: referenced_segments NOT updated here (conservative GC keeps all)
        // T2.2 will implement precise reference counting (rebuild set from remaining manifests)
    }

    /// Find snapshot by tag (O(1) lookup)
    ///
    /// Complexity: O(1) — HashMap lookup
    pub fn find_by_tag(&self, tag_key: &str, tag_value: &str) -> Option<u64> {
        self.tag_index
            .get(tag_key)
            .and_then(|values| values.get(tag_value))
            .copied()
    }

    /// List snapshots (O(N) where N = matching snapshots, not O(M×S))
    ///
    /// Complexity: O(N) where N = snapshots in index (in-memory filter)
    pub fn list_snapshots(&self, filter_tag: Option<&str>) -> Vec<SnapshotInfo> {
        if let Some(tag_key) = filter_tag {
            self.snapshots
                .iter()
                .filter(|info| info.tags.contains_key(tag_key))
                .cloned()
                .collect()
        } else {
            self.snapshots.clone()
        }
    }
}
```

**Index size:**
- 1K snapshots × ~10 bytes/snapshot = ~10 KB
- 10K snapshots = ~100 KB
- 100K snapshots = ~1 MB (still fast to load on SSD)

**Index corruption recovery:** If index gets out of sync with manifests, can rebuild by scanning `manifests/` directory (one-time repair operation).

### 1.5 CurrentPointer (Unchanged)

```rust
/// Atomic pointer to current manifest version.
///
/// Stored in `current.json` at database root. Updated via atomic rename
/// (write to `current.json.tmp`, then rename to `current.json`).
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub struct CurrentPointer {
    /// Current manifest version
    pub version: u64,
}

impl CurrentPointer {
    pub fn new(version: u64) -> Self {
        Self { version }
    }

    /// Read current pointer from database root.
    pub fn read_from(db_path: &Path) -> Result<Self> {
        let path = db_path.join("current.json");
        read_json(&path)
    }

    /// Write current pointer atomically.
    /// Uses temp file + rename pattern for atomicity.
    pub fn write_to(&self, db_path: &Path, durability: DurabilityMode) -> Result<()> {
        let path = db_path.join("current.json");
        atomic_write_json(&path, self, durability)?;

        if durability == DurabilityMode::Strict {
            fsync_directory(db_path)?; // Persist directory entry
        }

        Ok(())
    }
}
```

### 1.6 DurabilityMode (NEW: Configurable Fsync)

```rust
/// Durability mode for manifest writes.
///
/// Strict: Full fsync protocol (manifest + current pointer + directory).
///         Ensures crash safety at cost of ~5-10ms commit latency.
///
/// Relaxed: Skip fsync (OS buffers writes). Best-effort durability.
///          Faster commits (~1ms), but crash may lose recent commits.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DurabilityMode {
    Strict,  // fsync everything
    Relaxed, // skip fsync (OS handles flush)
}

impl Default for DurabilityMode {
    fn default() -> Self {
        DurabilityMode::Strict // Safe default
    }
}
```

### 1.7 SnapshotInfo (Unchanged)

```rust
/// Lightweight snapshot information for list operations.
///
/// Does NOT include full segment lists (saves memory when listing thousands of snapshots).
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SnapshotInfo {
    pub version: u64,
    pub created_at: u64,
    pub tags: HashMap<String, String>,
    pub stats: ManifestStats,
}

impl SnapshotInfo {
    /// Extract snapshot info from full manifest.
    pub fn from_manifest(manifest: &Manifest) -> Self {
        Self {
            version: manifest.version,
            created_at: manifest.created_at,
            tags: manifest.tags.clone(),
            stats: manifest.stats,
        }
    }
}
```

### 1.8 SnapshotDiff (Unchanged)

```rust
/// Diff between two snapshots (from_version → to_version).
///
/// Computed via HashSet-based set difference on segment IDs.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SnapshotDiff {
    pub from_version: u64,
    pub to_version: u64,

    /// Node segments added in to_version (not in from_version)
    pub added_node_segments: Vec<SegmentDescriptor>,

    /// Node segments removed in to_version (in from_version, not in to_version)
    pub removed_node_segments: Vec<SegmentDescriptor>,

    /// Edge segments added in to_version
    pub added_edge_segments: Vec<SegmentDescriptor>,

    /// Edge segments removed in to_version
    pub removed_edge_segments: Vec<SegmentDescriptor>,

    /// Stats for from_version
    pub stats_from: ManifestStats,

    /// Stats for to_version
    pub stats_to: ManifestStats,
}

impl SnapshotDiff {
    /// Compute diff between two manifests.
    ///
    /// Algorithm: HashSet-based set difference.
    /// Complexity: O(S) where S = total segments in both manifests.
    pub fn compute(from: &Manifest, to: &Manifest) -> Self {
        let from_node_ids: HashSet<u64> = from
            .node_segments
            .iter()
            .map(|s| s.segment_id)
            .collect();
        let to_node_ids: HashSet<u64> = to.node_segments.iter().map(|s| s.segment_id).collect();

        let added_node_segments: Vec<SegmentDescriptor> = to
            .node_segments
            .iter()
            .filter(|s| !from_node_ids.contains(&s.segment_id))
            .cloned()
            .collect();

        let removed_node_segments: Vec<SegmentDescriptor> = from
            .node_segments
            .iter()
            .filter(|s| !to_node_ids.contains(&s.segment_id))
            .cloned()
            .collect();

        let from_edge_ids: HashSet<u64> = from
            .edge_segments
            .iter()
            .map(|s| s.segment_id)
            .collect();
        let to_edge_ids: HashSet<u64> = to.edge_segments.iter().map(|s| s.segment_id).collect();

        let added_edge_segments: Vec<SegmentDescriptor> = to
            .edge_segments
            .iter()
            .filter(|s| !from_edge_ids.contains(&s.segment_id))
            .cloned()
            .collect();

        let removed_edge_segments: Vec<SegmentDescriptor> = from
            .edge_segments
            .iter()
            .filter(|s| !to_edge_ids.contains(&s.segment_id))
            .cloned()
            .collect();

        Self {
            from_version: from.version,
            to_version: to.version,
            added_node_segments,
            removed_node_segments,
            added_edge_segments,
            removed_edge_segments,
            stats_from: from.stats,
            stats_to: to.stats,
        }
    }

    /// Number of segments changed (added + removed)
    pub fn change_count(&self) -> usize {
        self.added_node_segments.len()
            + self.removed_node_segments.len()
            + self.added_edge_segments.len()
            + self.removed_edge_segments.len()
    }

    /// Is this a no-op diff (no changes)?
    pub fn is_empty(&self) -> bool {
        self.change_count() == 0
    }
}
```

---

## 2. ManifestStore API (Complete Function Signatures)

### 2.1 Main Structure

```rust
/// ManifestStore: manages manifest chain + index + current pointer + segment ID allocation.
///
/// NOT `Send + Sync` by default (contains PathBuf, cached Manifest).
/// For multi-threaded access, wrap in Arc<Mutex<ManifestStore>>.
pub struct ManifestStore {
    /// Database root path (None for ephemeral databases)
    db_path: Option<PathBuf>,

    /// Current manifest (cached in memory)
    current: Manifest,

    /// Manifest index (cached in memory)
    index: ManifestIndex,

    /// Next segment ID to allocate (thread-safe atomic counter)
    next_segment_id: AtomicU64,

    /// Durability mode (Strict = fsync, Relaxed = no fsync)
    durability: DurabilityMode,
}
```

### 2.2 Constructor Methods

```rust
impl ManifestStore {
    /// Open existing database (load current manifest + index).
    ///
    /// Algorithm:
    /// 1. Read current.json → get version
    /// 2. Load manifests/{version:06}.json
    /// 3. Load manifest_index.json
    /// 4. Initialize next_segment_id = max(index.referenced_segments) + 1
    ///
    /// Complexity: O(S + I) where S = segments in current manifest, I = index size
    ///
    /// Errors:
    /// - Io: current.json, manifest, or index file missing/unreadable
    /// - Json: file corrupt
    /// - InvalidFormat: manifest validation failed
    pub fn open_with_config(db_path: &Path, durability: DurabilityMode) -> Result<Self> {
        // Load current.json → get version
        let current_pointer = CurrentPointer::read_from(db_path)?;
        let current = load_manifest_file(db_path, current_pointer.version)?;

        // Load manifest_index.json
        let index_path = db_path.join("manifest_index.json");
        let index: ManifestIndex = read_json(&index_path)?;

        // Compute next_segment_id from index.referenced_segments
        let max_segment_id = index.referenced_segments.iter().max().copied().unwrap_or(0);
        let next_segment_id = AtomicU64::new(max_segment_id + 1);

        Ok(Self {
            db_path: Some(db_path.to_path_buf()),
            current,
            index,
            next_segment_id,
            durability,
        })
    }

    /// Open with default durability (Strict)
    pub fn open(db_path: &Path) -> Result<Self> {
        Self::open_with_config(db_path, DurabilityMode::Strict)
    }

    /// Create new database (write first manifest + empty index, version 1).
    ///
    /// Algorithm:
    /// 1. Create directories: manifests/, segments/, gc/
    /// 2. Create empty manifest (version=1, no segments)
    /// 3. Create empty index
    /// 4. Write manifests/000001.json
    /// 5. Write manifest_index.json
    /// 6. Write current.json → version 1
    /// 7. Initialize next_segment_id = 1
    ///
    /// Complexity: O(1)
    ///
    /// Errors:
    /// - Io: directory creation failed, file write failed
    /// - InvalidFormat: db_path already contains current.json
    pub fn create(db_path: &Path) -> Result<Self> {
        Self::create_with_config(db_path, DurabilityMode::Strict)
    }

    pub fn create_with_config(db_path: &Path, durability: DurabilityMode) -> Result<Self> {
        // Check if database already exists
        if db_path.join("current.json").exists() {
            return Err(GraphError::InvalidFormat(
                "Database already exists at path".to_string(),
            ));
        }

        // Create directory structure
        std::fs::create_dir_all(db_path.join("manifests"))?;
        std::fs::create_dir_all(db_path.join("segments"))?;
        std::fs::create_dir_all(db_path.join("gc"))?;

        // Create empty manifest v1
        let manifest = Manifest {
            version: 1,
            created_at: current_timestamp(),
            node_segments: Vec::new(),
            edge_segments: Vec::new(),
            tags: HashMap::new(),
            stats: ManifestStats {
                total_nodes: 0,
                total_edges: 0,
                node_segment_count: 0,
                edge_segment_count: 0,
            },
            parent_version: None,
        };

        // Create empty index
        let mut index = ManifestIndex::new();
        index.add_snapshot(&manifest);

        // Write manifest file
        let manifest_path = manifest_file_path(db_path, 1);
        atomic_write_json(&manifest_path, &manifest, durability)?;

        // Write index file
        let index_path = db_path.join("manifest_index.json");
        atomic_write_json(&index_path, &index, durability)?;

        // Write current pointer
        let current_pointer = CurrentPointer::new(1);
        current_pointer.write_to(db_path, durability)?;

        Ok(Self {
            db_path: Some(db_path.to_path_buf()),
            current: manifest,
            index,
            next_segment_id: AtomicU64::new(1),
            durability,
        })
    }

    /// Create ephemeral store (in-memory, no disk writes).
    ///
    /// Used for:
    /// - Unit tests
    /// - Temporary analysis graphs
    /// - Query-only databases (no persistence)
    ///
    /// Complexity: O(1)
    pub fn ephemeral() -> Self {
        let manifest = Manifest {
            version: 1,
            created_at: current_timestamp(),
            node_segments: Vec::new(),
            edge_segments: Vec::new(),
            tags: HashMap::new(),
            stats: ManifestStats {
                total_nodes: 0,
                total_edges: 0,
                node_segment_count: 0,
                edge_segment_count: 0,
            },
            parent_version: None,
        };

        let mut index = ManifestIndex::new();
        index.add_snapshot(&manifest);

        Self {
            db_path: None,
            current: manifest,
            index,
            next_segment_id: AtomicU64::new(1),
            durability: DurabilityMode::Strict, // doesn't matter for ephemeral
        }
    }
}
```

### 2.3 Core Operations

```rust
impl ManifestStore {
    /// Get current manifest (borrowed reference).
    ///
    /// Complexity: O(1) (cached in memory)
    pub fn current(&self) -> &Manifest {
        &self.current
    }

    /// Create new manifest (not yet committed).
    ///
    /// Constructs manifest with:
    /// - version = current.version + 1
    /// - created_at = now
    /// - provided segments + tags
    /// - computed stats
    /// - parent_version = current.version
    ///
    /// Does NOT write to disk or update self.current (caller must call commit()).
    ///
    /// Complexity: O(S) where S = total segments (for computing stats)
    ///
    /// Errors:
    /// - InvalidFormat: segments contain duplicate segment_id
    pub fn create_manifest(
        &self,
        node_segments: Vec<SegmentDescriptor>,
        edge_segments: Vec<SegmentDescriptor>,
        tags: Option<HashMap<String, String>>,
    ) -> Result<Manifest> {
        // Validate: no duplicate segment IDs
        let mut seen_ids = HashSet::new();
        for seg in node_segments.iter().chain(edge_segments.iter()) {
            if !seen_ids.insert(seg.segment_id) {
                return Err(GraphError::InvalidFormat(format!(
                    "Duplicate segment_id: {}",
                    seg.segment_id
                )));
            }
        }

        let version = self.current.version + 1;
        let stats = ManifestStats::from_segments(&node_segments, &edge_segments);

        Ok(Manifest {
            version,
            created_at: current_timestamp(),
            node_segments,
            edge_segments,
            tags: tags.unwrap_or_default(),
            stats,
            parent_version: Some(self.current.version),
        })
    }

    /// Atomically commit manifest version (swap current pointer).
    ///
    /// Algorithm:
    /// 1. Verify manifest.version == current.version + 1 (monotonicity check)
    /// 2. Write manifests/{version:06}.json
    /// 3. Fsync manifest file (if Strict mode)
    /// 4. Update self.index in memory (add snapshot)
    /// 5. Write manifest_index.json.tmp
    /// 6. Fsync manifest_index.json.tmp (if Strict mode)
    /// 7. Atomic rename manifest_index.json.tmp → manifest_index.json
    /// 8. Write current.json.tmp with new version
    /// 9. Fsync current.json.tmp (if Strict mode)
    /// 10. Atomic rename current.json.tmp → current.json
    /// 11. Fsync directory (if Strict mode + Linux)
    /// 12. Update self.current cache
    ///
    /// Complexity: O(S + I) where S = segments in manifest, I = index size
    ///
    /// Errors:
    /// - InvalidFormat: version not monotonic, manifest already committed
    /// - Io: file write or fsync failed
    /// - Json: serialization failed
    ///
    /// If ephemeral: only update self.current + self.index, no disk writes.
    pub fn commit(&mut self, manifest: Manifest) -> Result<()> {
        // Ephemeral: just update cache
        if self.db_path.is_none() {
            self.index.add_snapshot(&manifest);
            self.current = manifest;
            return Ok(());
        }

        let db_path = self.db_path.as_ref().unwrap();

        // Monotonicity check
        if manifest.version != self.current.version + 1 {
            return Err(GraphError::InvalidFormat(format!(
                "Cannot commit version {} (current: {})",
                manifest.version, self.current.version
            )));
        }

        // 1. Write manifest file
        let manifest_path = manifest_file_path(db_path, manifest.version);
        atomic_write_json(&manifest_path, &manifest, self.durability)?;

        // 2. Update index in memory
        self.index.add_snapshot(&manifest);

        // 3. Write index file
        let index_path = db_path.join("manifest_index.json");
        atomic_write_json(&index_path, &self.index, self.durability)?;

        // 4. Write + rename current pointer
        let current_pointer = CurrentPointer::new(manifest.version);
        current_pointer.write_to(db_path, self.durability)?;

        // 5. Update cache
        self.current = manifest;

        Ok(())
    }

    /// Load specific manifest version from disk.
    ///
    /// Complexity: O(S) where S = segments in manifest (JSON deserialization)
    ///
    /// Errors:
    /// - Io: manifest file not found
    /// - Json: manifest file corrupt
    /// - InvalidFormat: manifest validation failed
    pub fn load_manifest(&self, version: u64) -> Result<Manifest> {
        if let Some(db_path) = &self.db_path {
            load_manifest_file(db_path, version)
        } else {
            // Ephemeral: only current manifest exists
            if version == self.current.version {
                Ok(self.current.clone())
            } else {
                Err(GraphError::InvalidFormat(format!(
                    "Ephemeral database has no version {}",
                    version
                )))
            }
        }
    }

    /// Allocate next segment ID (thread-safe).
    ///
    /// Uses atomic fetch_add for lock-free concurrency.
    ///
    /// Complexity: O(1)
    pub fn next_segment_id(&self) -> u64 {
        self.next_segment_id.fetch_add(1, Ordering::SeqCst)
    }
}
```

### 2.4 Snapshot Operations

```rust
impl ManifestStore {
    /// Find snapshot by tag (O(1) index lookup).
    ///
    /// Algorithm:
    /// 1. Check self.index.tag_index[tag_key][tag_value]
    /// 2. Return version if found, None otherwise
    ///
    /// Complexity: O(1) — HashMap lookup
    ///
    /// Errors: None (returns Option)
    pub fn find_snapshot(&self, tag_key: &str, tag_value: &str) -> Option<u64> {
        self.index.find_by_tag(tag_key, tag_value)
    }

    /// List all snapshots (optionally filtered by tag key).
    ///
    /// Algorithm:
    /// 1. Check self.index.snapshots (already in memory)
    /// 2. Filter by tag_key if provided
    /// 3. Return Vec<SnapshotInfo>
    ///
    /// Complexity: O(N) where N = snapshots in index (in-memory filter)
    ///
    /// Errors: None
    pub fn list_snapshots(&self, filter_tag: Option<&str>) -> Vec<SnapshotInfo> {
        self.index.list_snapshots(filter_tag)
    }

    /// Tag existing snapshot (modifies manifest file atomically, updates index).
    ///
    /// Algorithm:
    /// 1. Load manifest at version
    /// 2. Merge new tags into manifest.tags (overwrite existing keys)
    /// 3. Write manifest atomically (temp file + rename)
    /// 4. Update index.tag_index
    /// 5. Write index atomically
    ///
    /// Complexity: O(S + I) where S = segments in manifest, I = index size
    ///
    /// Safety: Atomic write ensures no torn reads. Only tags are modified
    /// (segments/stats/version unchanged).
    ///
    /// Errors:
    /// - Io: manifest file not found or write failed
    /// - Json: manifest file corrupt or serialization failed
    pub fn tag_snapshot(&mut self, version: u64, tags: HashMap<String, String>) -> Result<()> {
        if self.db_path.is_none() {
            return Err(GraphError::InvalidFormat(
                "Cannot tag ephemeral snapshot".to_string(),
            ));
        }

        let db_path = self.db_path.as_ref().unwrap();
        let manifest_path = manifest_file_path(db_path, version);

        // Load existing manifest
        let mut manifest: Manifest = read_json(&manifest_path)?;

        // Merge tags (new tags overwrite old)
        for (key, value) in &tags {
            manifest.tags.insert(key.clone(), value.clone());
        }

        // Write manifest atomically
        atomic_write_json(&manifest_path, &manifest, self.durability)?;

        // Update index
        for (key, value) in tags {
            self.index
                .tag_index
                .entry(key)
                .or_insert_with(HashMap::new)
                .insert(value, version);
        }

        // Write index atomically
        let index_path = db_path.join("manifest_index.json");
        atomic_write_json(&index_path, &self.index, self.durability)?;

        // If current manifest, update cache
        if version == self.current.version {
            self.current = manifest;
        }

        Ok(())
    }

    /// Compute diff between two snapshots.
    ///
    /// Complexity: O(S) where S = total segments in both manifests
    ///
    /// Errors:
    /// - Io: manifest file not found
    /// - Json: manifest file corrupt
    pub fn diff_snapshots(&self, from_version: u64, to_version: u64) -> Result<SnapshotDiff> {
        let from = self.load_manifest(from_version)?;
        let to = self.load_manifest(to_version)?;
        Ok(SnapshotDiff::compute(&from, &to))
    }
}
```

### 2.5 Garbage Collection Operations

```rust
impl ManifestStore {
    /// Collect unreferenced segments (move to gc/ directory).
    ///
    /// Algorithm:
    /// 1. Check self.index.referenced_segments (already in memory)
    /// 2. Scan segments/ directory → find files not in referenced set
    /// 3. Move unreferenced files to gc/ directory (preserve filename)
    /// 4. Return list of moved file paths
    ///
    /// Complexity: O(F) where F = files in segments/ directory
    /// (No manifest loading needed — index tracks referenced segments)
    ///
    /// Safety: Two-phase GC (collect → purge). If logic is wrong, files are
    /// in gc/ (recoverable), not deleted (permanent).
    ///
    /// Errors:
    /// - Io: directory read/write failed, file move failed
    pub fn gc_collect(&self) -> Result<Vec<String>> {
        if self.db_path.is_none() {
            return Ok(Vec::new()); // ephemeral: no GC
        }

        let db_path = self.db_path.as_ref().unwrap();
        let segments_dir = db_path.join("segments");
        let gc_dir = db_path.join("gc");

        // Ensure gc/ directory exists
        std::fs::create_dir_all(&gc_dir)?;

        // Referenced segments already tracked in index
        let referenced_ids = &self.index.referenced_segments;

        // Scan segments/ directory
        let mut moved = Vec::new();

        for entry in std::fs::read_dir(&segments_dir)? {
            let entry = entry?;
            let path = entry.path();

            if path.extension().and_then(|s| s.to_str()) != Some("seg") {
                continue; // skip non-segment files
            }

            // Extract segment_id from filename: seg_{id:06}_{type}.seg
            if let Some(segment_id) =
                parse_segment_id_from_filename(path.file_name().unwrap().to_str().unwrap())
            {
                if !referenced_ids.contains(&segment_id) {
                    // Move to gc/
                    let filename = path.file_name().unwrap();
                    let gc_path = gc_dir.join(filename);
                    std::fs::rename(&path, &gc_path)?;
                    moved.push(gc_path.to_string_lossy().to_string());
                }
            }
        }

        Ok(moved)
    }

    /// Purge files from gc/ directory (permanent deletion).
    ///
    /// Algorithm:
    /// 1. Scan gc/ directory
    /// 2. Delete all .seg files
    /// 3. Return count of deleted files
    ///
    /// Complexity: O(F) where F = files in gc/
    ///
    /// Safety: Only deletes files in gc/ (already unreferenced by collect()).
    ///
    /// Errors:
    /// - Io: directory read or file deletion failed
    pub fn gc_purge(&self) -> Result<usize> {
        if self.db_path.is_none() {
            return Ok(0); // ephemeral: no GC
        }

        let db_path = self.db_path.as_ref().unwrap();
        let gc_dir = db_path.join("gc");

        let mut deleted = 0;

        for entry in std::fs::read_dir(&gc_dir)? {
            let entry = entry?;
            let path = entry.path();

            if path.extension().and_then(|s| s.to_str()) != Some("seg") {
                continue; // skip non-segment files (e.g., gc_log.json)
            }

            std::fs::remove_file(&path)?;
            deleted += 1;
        }

        Ok(deleted)
    }
}
```

---

## 3. Helper Functions (File I/O)

### 3.1 Atomic Write with Conditional Fsync

```rust
/// Write JSON to file atomically via temp file + rename.
///
/// Algorithm:
/// 1. Write to {path}.tmp
/// 2. Fsync temp file (if Strict mode)
/// 3. Atomic rename {path}.tmp → {path}
/// 4. Return success
///
/// Atomicity: Rename is atomic on POSIX (single syscall).
/// On Windows: rename is NOT atomic if target exists. Must use fs2 crate
/// or ReplaceFile API (TODO: T2.2).
///
/// Complexity: O(N) where N = serialized JSON size
///
/// Errors:
/// - Io: file write failed, fsync failed, rename failed
/// - Json: serialization failed
fn atomic_write_json<T: Serialize>(
    path: &Path,
    data: &T,
    durability: DurabilityMode,
) -> Result<()> {
    let temp_path = path.with_extension("tmp");

    // Write to temp file
    let mut file = File::create(&temp_path)?;
    serde_json::to_writer_pretty(&file, data)?;

    // Fsync if Strict mode
    if durability == DurabilityMode::Strict {
        file.sync_all()?;
    }

    // Atomic rename
    std::fs::rename(&temp_path, path)?;

    Ok(())
}
```

**Why pretty print?** Manifests are debugging artifacts. Pretty JSON aids manual inspection. Cost: ~20% larger files, negligible for <5KB manifests.

### 3.2 Read JSON

```rust
/// Read JSON from file.
///
/// Complexity: O(N) where N = file size
///
/// Errors:
/// - Io: file not found or unreadable
/// - Json: invalid JSON or deserialization failed
fn read_json<T: DeserializeOwned>(path: &Path) -> Result<T> {
    let file = File::open(path)?;
    Ok(serde_json::from_reader(file)?)
}
```

### 3.3 Directory Fsync (Durability on Linux)

```rust
/// Fsync directory to persist directory entry changes.
///
/// Required after rename operations on ext4/XFS to ensure directory
/// metadata is flushed to disk.
///
/// Platform-specific:
/// - Linux: required for durability
/// - macOS: no-op (HFS+/APFS auto-persist directory metadata)
/// - Windows: no-op (NTFS auto-persist)
///
/// Complexity: O(1) (single fsync syscall)
///
/// Errors:
/// - Io: directory open or fsync failed
#[cfg(target_os = "linux")]
fn fsync_directory(path: &Path) -> Result<()> {
    let dir = File::open(path)?;
    dir.sync_all()?;
    Ok(())
}

#[cfg(not(target_os = "linux"))]
fn fsync_directory(_path: &Path) -> Result<()> {
    Ok(()) // no-op on macOS/Windows
}
```

### 3.4 Utility Functions

```rust
/// Get manifest file path: {db_path}/manifests/{version:06}.json
fn manifest_file_path(db_path: &Path, version: u64) -> PathBuf {
    db_path
        .join("manifests")
        .join(format!("{:06}.json", version))
}

/// Load manifest file from disk with validation.
///
/// Helper function used by open() and load_manifest().
fn load_manifest_file(db_path: &Path, version: u64) -> Result<Manifest> {
    let path = manifest_file_path(db_path, version);
    let manifest: Manifest = read_json(&path)?;

    // Validate loaded manifest
    if manifest.version != version {
        return Err(GraphError::InvalidFormat(format!(
            "Manifest version mismatch: expected {}, got {}",
            version, manifest.version
        )));
    }

    #[cfg(debug_assertions)]
    manifest
        .stats
        .validate(&manifest.node_segments, &manifest.edge_segments);

    Ok(manifest)
}

/// Parse segment ID from filename: seg_000123_nodes.seg → 123
fn parse_segment_id_from_filename(filename: &str) -> Option<u64> {
    // Pattern: seg_{id:06}_{type}.seg
    let parts: Vec<&str> = filename.split('_').collect();
    if parts.len() >= 3 && parts[0] == "seg" {
        parts[1].parse::<u64>().ok()
    } else {
        None
    }
}

/// Get current Unix timestamp (seconds since epoch)
fn current_timestamp() -> u64 {
    std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs()
}
```

---

## 4. Big-O Complexity Analysis

### 4.1 Read Operations (IMPROVED)

| Operation | Old | New | Notes |
|-----------|-----|-----|-------|
| `open()` | O(S) | **O(S + I)** | S = segments in current, I = index size (single file) |
| `current()` | O(1) | **O(1)** | Cached in memory |
| `load_manifest(v)` | O(S) | **O(S)** | S = segments in manifest v (JSON parse) |
| `find_snapshot(tag)` | O(N×S) | **O(1)** | HashMap lookup in index.tag_index ✅ |
| `list_snapshots()` | O(M×S) | **O(N)** | N = snapshots in index (in-memory) ✅ |
| `diff_snapshots(from, to)` | O(S) | **O(S)** | S = total segments in both manifests |

### 4.2 Write Operations

| Operation | Complexity | Notes |
|-----------|-----------|-------|
| `create()` | O(1) | Write empty manifest + empty index + current pointer |
| `create_manifest()` | O(S) | S = segments (compute stats, validate IDs) |
| `commit(manifest)` | O(S + I) | S = segments (JSON serialize), I = index size (update + write) |
| `tag_snapshot(v)` | O(S + I) | S = segments in manifest v, I = index size (rewrite both) |
| `next_segment_id()` | O(1) | Atomic fetch_add |

### 4.3 GC Operations (IMPROVED)

| Operation | Old | New | Notes |
|-----------|-----|-----|-------|
| `gc_collect()` | O(R×S + F) | **O(F)** | F = files in segments/, no manifest loading ✅ |
| `gc_purge()` | O(F) | **O(F)** | F = files in gc/ |

### 4.4 Space Complexity

| Data Structure | Size | Notes |
|---------------|------|-------|
| `Manifest` in memory | O(S) | S = segments (Vec<SegmentDescriptor>) |
| `Manifest` on disk | ~1-5 KB | JSON, 100 segments × ~30 bytes each + metadata |
| `ManifestIndex` in memory | O(M + T + R) | M = snapshots, T = tag entries, R = referenced segments |
| `ManifestIndex` on disk | ~10 KB per 1K snapshots | JSON, compresses well |
| `CurrentPointer` | 20 bytes | JSON: `{"version": 12345}` |
| `ManifestStore` | O(S + I) | Cached current manifest + index |

### 4.5 Performance Targets (Phase 1)

- **Commit latency (Strict):** <10ms for 100-segment manifest on SSD
- **Commit latency (Relaxed):** <1ms for 100-segment manifest on SSD
- **GC collect:** <100ms for 10K segments (no manifest loading)
- **List snapshots:** <5ms for 1000 snapshots (in-memory filter)
- **Find snapshot by tag:** <1ms (HashMap lookup)

These are NOT hard requirements (no premature optimization), but ballpark targets to sanity-check algorithm choices.

---

## 5. Updated Implementation Roadmap

### Phase 1: Data Structures + Serde (120 LOC, 7 tests)

**File:** `storage_v2/manifest.rs`

**Tasks:**
1. Define all structs with serde derives:
   - `Manifest`, `SegmentDescriptor`, `SegmentType`, `ManifestStats`, `CurrentPointer`
   - `ManifestIndex` (NEW)
   - `DurabilityMode` (NEW)
   - `SnapshotInfo`, `SnapshotDiff`
2. Implement `SegmentDescriptor::from_meta()`
3. Implement `SegmentDescriptor::file_path()` (NEW: derived paths)
4. Implement `SegmentDescriptor::relative_path()` (NEW)
5. Implement `ManifestStats::from_segments()`
6. Implement `ManifestIndex::new()`, `add_snapshot()`, `find_by_tag()`, `list_snapshots()`
7. Implement `SnapshotDiff::compute()`

**Tests:**
- `test_manifest_serde_roundtrip`: Serialize → deserialize → identical
- `test_manifest_stats_computation`: Segments → stats matches
- `test_segment_descriptor_from_meta`: SegmentMeta → SegmentDescriptor
- `test_segment_descriptor_file_path_flat`: shard_id = None → "segments/seg_000001_nodes.seg"
- `test_segment_descriptor_file_path_sharded`: shard_id = Some(5) → "segments/05/seg_000001_nodes.seg"
- `test_manifest_index_add_snapshot`: Add 3 snapshots → verify index state
- `test_manifest_index_find_by_tag`: Add tagged snapshot → find_by_tag returns version

**Expected output:** All structs compile, serde works, basic helper methods implemented.

### Phase 2: File I/O Helpers (100 LOC, 6 tests)

**File:** `storage_v2/manifest.rs` (add helper functions)

**Tasks:**
1. Implement `atomic_write_json()` with DurabilityMode parameter (NEW: conditional fsync)
2. Implement `read_json()`
3. Implement `fsync_directory()` (conditional compilation)
4. Implement `manifest_file_path()`, `load_manifest_file()`, `parse_segment_id_from_filename()`, `current_timestamp()`

**Tests:**
- `test_atomic_write_json_strict_mode`: Write with fsync → verify file exists
- `test_atomic_write_json_relaxed_mode`: Write without fsync → verify file exists
- `test_read_json_missing_file`: Returns Io error
- `test_fsync_directory_no_error`: Call on temp dir, no crash
- `test_parse_segment_id_from_filename`: Various formats
- `test_durability_mode_default`: Verify default is Strict

**Expected output:** Atomic write works with conditional fsync, directory fsync compiles on all platforms.

### Phase 3: ManifestStore Core (180 LOC, 10 tests)

**File:** `storage_v2/manifest.rs` (add ManifestStore impl)

**Tasks:**
1. Implement `ManifestStore::ephemeral()`
2. Implement `ManifestStore::create()` and `create_with_config()` (NEW: write index)
3. Implement `ManifestStore::open()` and `open_with_config()` (NEW: load index)
4. Implement `ManifestStore::create_manifest()`
5. Implement `ManifestStore::commit()` (NEW: update + write index)
6. Implement `ManifestStore::load_manifest()`
7. Implement `ManifestStore::next_segment_id()`
8. Implement `CurrentPointer::read_from()` and `CurrentPointer::write_to()`

**Tests:**
- `test_manifest_store_ephemeral`: Create ephemeral, verify no db_path
- `test_manifest_store_create_new_database`: Create → verify files exist
- `test_manifest_store_create_writes_index`: Create → verify manifest_index.json exists
- `test_manifest_store_open_existing`: Create → close → reopen → same version
- `test_manifest_store_open_loads_index`: Reopen → verify index.snapshots populated
- `test_manifest_store_commit_updates_index`: Commit v2 → verify index.snapshots has 2 entries
- `test_manifest_store_commit_sequential`: Create v1 → commit v2 → commit v3
- `test_manifest_store_commit_monotonicity_check`: Try commit v1 again → error
- `test_manifest_store_load_manifest`: Commit v2 → load v1 → correct segments
- `test_manifest_store_next_segment_id_increments`: Allocate 3 IDs → 1, 2, 3

**Expected output:** Can create database, commit manifests, reopen database. Atomic commit protocol works. Index tracks all snapshots.

### Phase 4: Snapshot Operations (100 LOC, 6 tests)

**File:** `storage_v2/manifest.rs` (extend ManifestStore impl)

**Tasks:**
1. Implement `ManifestStore::find_snapshot()` (NEW: via index)
2. Implement `ManifestStore::list_snapshots()` (NEW: via index)
3. Implement `ManifestStore::tag_snapshot()` (NEW: update index)
4. Implement `SnapshotInfo::from_manifest()`

**Tests:**
- `test_find_snapshot_by_tag_via_index`: Tag v2 → find → returns 2 (verify no manifest loading)
- `test_find_snapshot_not_found`: Search nonexistent tag → None
- `test_list_snapshots_all_via_index`: Create 3 versions → list → 3 entries (verify no manifest loading)
- `test_list_snapshots_filtered_by_tag`: Tag v2, list with filter → only v2
- `test_tag_snapshot_updates_index`: Tag v1 → verify index.tag_index updated
- `test_tag_snapshot_persists_after_reopen`: Tag v1 → reopen → tag still present

**Expected output:** Can tag snapshots, find by tag (O(1)), list all snapshots (O(N) in-memory).

### Phase 5: Diff Computation (50 LOC, 4 tests)

**File:** Already implemented in `SnapshotDiff::compute()` (Phase 1)

**Tasks:**
1. Add tests for diff computation edge cases

**Tests:**
- `test_diff_empty_to_populated`: v1 (0 segments) → v2 (3 segments)
- `test_diff_same_version`: v1 → v1 diff is empty
- `test_diff_mixed_changes`: v1 (3 segs) → v2 (2 removed, 3 added)
- `test_diff_stats_match`: Verify stats_from/stats_to correct

**Expected output:** Diff logic handles all edge cases correctly.

### Phase 6: Garbage Collection (100 LOC, 5 tests)

**File:** `storage_v2/manifest.rs` (extend ManifestStore impl)

**Tasks:**
1. Implement `ManifestStore::gc_collect()` (NEW: via index.referenced_segments)
2. Implement `ManifestStore::gc_purge()`

**Tests:**
- `test_gc_collect_uses_index`: Create v1-v10 → GC → verify no manifest loading (measure via mock/counter)
- `test_gc_collect_unreferenced`: Create v1-v10 → GC → old segments in gc/
- `test_gc_collect_preserves_referenced`: Create v1-v10 → GC → recent segments untouched
- `test_gc_purge_deletes_files`: Move 3 files to gc/ → purge → 3 deleted
- `test_gc_ephemeral_no_op`: Ephemeral store → gc_collect → returns empty vec

**Expected output:** GC correctly identifies unreferenced segments (O(F)), two-phase collect/purge works.

### Phase 7: Integration Tests (100 LOC, 7 tests)

**File:** `tests/integration/manifest_store_tests.rs`

**Tasks:**
1. End-to-end test: create → commit → reopen → verify
2. Crash simulation: kill mid-write → verify DB still opens
3. Index consistency after crash: partial commit → verify index matches current pointer
4. Concurrent reads: spawn reader thread, commit new manifest, verify reader sees old snapshot
5. Large manifest: 1000 segments → commit → reopen → verify
6. GC integration: create 100 versions → GC → verify
7. Tag + diff workflow: tag multiple versions, find, diff, verify

**Tests:**
- `test_end_to_end_create_commit_reopen`
- `test_crash_simulation_partial_write`
- `test_index_consistency_after_crash` (NEW)
- `test_concurrent_reader_isolation`
- `test_large_manifest_1000_segments`
- `test_gc_with_no_retention` (NEW: Phase 1 has no retention parameter)
- `test_tag_find_diff_workflow`

**Expected output:** All integration tests pass. System works end-to-end.

---

## 6. Updated Storage Layout

```
<name>.rfdb/
├── current.json                    # Atomic pointer: {"version": 5}
├── manifest_index.json             # ✅ NEW: Index with all snapshot metadata + tag index + referenced segments
├── manifests/
│   ├── 000001.json                 # Manifest v1 (immutable after commit)
│   ├── 000002.json
│   ├── 000003.json
│   └── 000005.json                 # Current (gaps OK after crash recovery)
├── segments/
│   ├── seg_000001_nodes.seg        # Immutable v2 segment
│   ├── seg_000001_edges.seg
│   ├── seg_000002_nodes.seg
│   └── ...
└── gc/                             # Segments pending deletion
    └── seg_000001_nodes.seg
```

**Example manifest_index.json:**
```json
{
  "latest_version": 5,
  "snapshots": [
    {
      "version": 1,
      "created_at": 1707826800,
      "tags": {},
      "stats": {
        "total_nodes": 0,
        "total_edges": 0,
        "node_segment_count": 0,
        "edge_segment_count": 0
      }
    },
    {
      "version": 5,
      "created_at": 1707826805,
      "tags": {
        "commit_sha": "abc123",
        "analysis_run": "success"
      },
      "stats": {
        "total_nodes": 2300,
        "total_edges": 2300,
        "node_segment_count": 2,
        "edge_segment_count": 1
      }
    }
  ],
  "tag_index": {
    "commit_sha": {
      "abc123": 5
    },
    "analysis_run": {
      "success": 5
    }
  },
  "referenced_segments": [1, 2, 3, 4]
}
```

---

## 7. Updated Commit Protocol Diagram

```
Commit Flow (v1 → v2):

1. Prepare
   └─ create_manifest() → Manifest v2 (in memory)

2. Write Manifest
   ├─ Write manifests/000002.json.tmp
   ├─ Fsync manifests/000002.json.tmp (if Strict)
   └─ Rename .tmp → manifests/000002.json

3. Update Index (in memory)
   └─ self.index.add_snapshot(&manifest_v2)

4. Write Index
   ├─ Write manifest_index.json.tmp
   ├─ Fsync manifest_index.json.tmp (if Strict)
   └─ Rename .tmp → manifest_index.json

5. Update Current Pointer (ATOMIC)
   ├─ Write current.json.tmp ({"version": 2})
   ├─ Fsync current.json.tmp (if Strict)
   ├─ Rename current.json.tmp → current.json
   └─ Fsync directory (if Strict + Linux)

6. Update Cache
   └─ self.current = manifest_v2

Result: Either old version active (crash before step 5) or new version active.
Index always consistent with current pointer (both updated atomically).
```

---

## 8. Updated GC Algorithm Diagram

```
GC Flow (no retention parameter in Phase 1):

Current version: 10
Referenced segments: index.referenced_segments = {10, 11, 12, 13, 14}

1. Get Referenced Segments
   └─ referenced = &self.index.referenced_segments  (✅ O(1) — already in memory)

2. Scan segments/ Directory
   Files: seg_000001_nodes.seg, seg_000002_nodes.seg, ..., seg_000014_nodes.seg
   Unreferenced: {1, 2, 3, 4, 5, 6, 7, 8, 9}

3. Move to gc/
   ├─ mv segments/seg_000001_nodes.seg → gc/seg_000001_nodes.seg
   ├─ mv segments/seg_000002_nodes.seg → gc/seg_000002_nodes.seg
   └─ ...

4. Purge (separate call)
   ├─ rm gc/seg_000001_nodes.seg
   ├─ rm gc/seg_000002_nodes.seg
   └─ ...

Safety: If step 1-2 logic wrong → files in gc/, not deleted.
Complexity: O(F) where F = files in segments/ directory (no manifest loading).
```

---

## 9. What NOT To Do (Updated)

### 9.1 Still Deferred to Later Tasks

1. **Sharding implementation (T2.2):** Add sharding config, generate `shard_id` for new segments. Phase 1: `shard_id = None` always.
2. **Compaction (T3.2):** Merge segments. Phase 1: no compaction.
3. **Tombstones (T2.2):** Delete records. Phase 1: no deletes.
4. **Inverted Index (T4.x):** Global index files. Phase 1: no index.
5. **Manifest compression (future):** gzip manifests. Phase 1: plain JSON.
6. **Multi-part index (future):** Split index for >100K snapshots. Phase 1: single file.
7. **Manifest GC (T2.2):** Remove old manifests from index. Phase 1: keep all manifests forever.

### 9.2 What the Index Does NOT Do

1. **No time-based GC retention:** Phase 1 GC collects ALL unreferenced segments immediately. Time-based retention (keep for N days) deferred to T2.2.
2. **No per-segment metadata:** Index tracks segment IDs, not per-segment metadata (that's in manifest). Keeps index small.
3. **No query planning:** Index is for manifest operations, not graph queries. Query planning uses manifest zone maps (unchanged).

### 9.3 Anti-Patterns to Avoid

1. **Mutable manifests (except tags):**
   - Manifests are IMMUTABLE after commit
   - Only exception: `tag_snapshot()` modifies tags atomically
   - DO NOT allow editing segments/stats/version after commit

2. **Blocking GC:**
   - GC must NOT delete files immediately
   - Use two-phase: collect (move to gc/) → purge (delete from gc/)
   - If collect logic is wrong, files recoverable from gc/

3. **Non-atomic writes:**
   - Always use atomic_write_json (temp file + rename)
   - Never write directly to manifest/current/index files

4. **Skipping fsync in Strict mode:**
   - Always fsync manifest before updating index (if Strict)
   - Always fsync index before updating current pointer (if Strict)
   - Always fsync current.json.tmp before rename (if Strict)
   - Always fsync directory after rename (if Strict + Linux)

5. **Index out of sync:**
   - Index MUST be updated during every commit (before current pointer)
   - If crash happens, old index + old current pointer = consistent

6. **Over-engineering:**
   - Keep it simple: single file, ~750 LOC
   - Don't add features not in this spec
   - Don't optimize prematurely (no per-manifest cache, no background GC)

---

## 10. Success Criteria (Updated)

### 10.1 Functional Requirements

- [ ] Can create new database with first manifest (v1) + empty index
- [ ] Index correctly updated during commit
- [ ] Index persists across reopen
- [ ] Can commit sequential manifests (v1 → v2 → v3)
- [ ] Can reopen database and load current manifest + index
- [ ] Can tag snapshots (updates both manifest and index)
- [ ] **find_snapshot() uses index (O(1) lookup, no manifest loading)**
- [ ] **list_snapshots() uses index (O(N) in-memory, no manifest loading)**
- [ ] Can compute diff between two snapshots
- [ ] **gc_collect() uses index.referenced_segments (O(F), no manifest loading)**
- [ ] Can purge gc/ directory
- [ ] SegmentDescriptor.file_path() works with shard_id = None
- [ ] SegmentDescriptor.file_path() works with shard_id = Some (future-proofing)
- [ ] DurabilityMode::Strict performs fsync
- [ ] DurabilityMode::Relaxed skips fsync
- [ ] Atomic commit works (crash mid-write → DB still opens)
- [ ] Index consistency after crash (partial commit → index matches current pointer)
- [ ] Concurrent readers isolated (reader sees old snapshot during writer commit)

### 10.2 Test Coverage

- [ ] All ~45 tests pass
- [ ] Crash simulation test passes (kill mid-write)
- [ ] Index consistency test passes (partial commit → index + current consistent)
- [ ] Concurrent reader test passes (isolation verified)
- [ ] GC uses index test passes (no manifest loading during GC)

### 10.3 Code Quality

- [ ] No clippy warnings (run `cargo clippy --all-targets`)
- [ ] No panics in production code (except debug_assert)
- [ ] All public APIs documented with `///` comments
- [ ] All error paths tested (missing files, corrupt JSON, etc.)
- [ ] No TODO/FIXME/HACK comments (except planned future work)

### 10.4 Performance

- [ ] Commit latency <10ms (Strict mode, 100 segments, SSD)
- [ ] Commit latency <1ms (Relaxed mode, 100 segments, SSD)
- [ ] GC collect <100ms for 10K segments (no manifest loading)
- [ ] list_snapshots <5ms for 1000 snapshots (in-memory filter)
- [ ] find_snapshot <1ms (HashMap lookup)

---

## 11. Module Structure & Exports

### 11.1 File Organization

**Phase 1:** Single file `storage_v2/manifest.rs` (~750 LOC).

**Future (if >800 LOC):** Split into:
- `storage_v2/manifest/types.rs` — data structures
- `storage_v2/manifest/store.rs` — ManifestStore implementation
- `storage_v2/manifest/diff.rs` — diff computation
- `storage_v2/manifest/mod.rs` — re-exports

### 11.2 Module Exports

Update `packages/rfdb-server/src/storage_v2/mod.rs`:

```rust
pub mod types;
pub mod string_table;
pub mod bloom;
pub mod zone_map;
pub mod writer;
pub mod segment;
pub mod manifest; // NEW

pub use types::*;
pub use string_table::StringTableV2;
pub use bloom::BloomFilter;
pub use zone_map::ZoneMap;
pub use writer::{NodeSegmentWriter, EdgeSegmentWriter};
pub use segment::{NodeSegmentV2, EdgeSegmentV2};

// NEW: Manifest exports
pub use manifest::{
    Manifest,
    ManifestStore,
    SegmentDescriptor,
    SegmentType,
    ManifestStats,
    ManifestIndex,
    CurrentPointer,
    DurabilityMode,
    SnapshotInfo,
    SnapshotDiff,
};
```

---

## 12. Dependencies (No New Crates)

Already in `Cargo.toml`:
```toml
[dependencies]
serde = { version = "1", features = ["derive"] }
serde_json = "1"
blake3 = "1"
```

No new dependencies required for Phase 1.

---

## 13. Summary for Rob Pike (Implementation)

**You will receive this spec and implement Phase 1-7 in order.**

**Key changes from original spec:**

1. **ManifestIndex added** — tracks all snapshots + tag index + referenced segments
2. **SegmentDescriptor.file_path() derived** — not stored as string, enables sharding
3. **DurabilityMode added** — configurable fsync (Strict vs Relaxed)
4. **Commit protocol updated** — write manifest → update index → write index → update current
5. **GC simplified** — O(F) via index.referenced_segments, no manifest loading
6. **find_snapshot simplified** — O(1) via index.tag_index, no chain traversal
7. **list_snapshots simplified** — O(N) in-memory filter, no directory scanning

**Critical invariants to maintain:**
- Manifests immutable after commit (except tags)
- Index updated BEFORE current pointer (atomic commit marker)
- Version numbers strictly increasing (gaps OK, duplicates forbidden)
- Stats must match segment descriptors (validated in debug builds)
- Atomic write: temp file + fsync (if Strict) + rename + fsync directory (if Strict + Linux)
- GC: two-phase (collect → purge), never delete directly
- Index consistency: if crash during commit, old index + old current = consistent

**When stuck:**
- Read Don's revision again (architectural context)
- Check existing code (`storage_v2/types.rs`, `writer.rs`)
- Ask questions BEFORE writing code (no blind guessing)

**Success = all ~45 tests pass + clippy clean + manual smoke test.**

---

**End of Specification v2**

This document is complete and ready for implementation. Rob Pike should follow phases sequentially. Kent Beck should write tests first (TDD). Kevlin Henney will review code quality after each phase.
